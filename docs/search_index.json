[["index.html", "Time Series Analysis Chapter 1 About", " Time Series Analysis shang-chieh0830 2023-03-14 Chapter 1 About This book is a concise lecture note about Time Series Analysis. The content of this book is from the course Time Series Analysis taught by Chris Bilder. You can check his YouTube channel to get full(and correct) information about this course. Again, I do NOT own the content of this book. I write this book only for studying. All credits belong to Chris Bilder. If there is any copyright concerns, I will make this book private ASAP. "],["introduction-to-r.html", "Chapter 2 Introduction to R 2.1 Basic Operation 2.2 Vectors 2.3 Files 2.4 Regression 2.5 Object-Oriented Language", " Chapter 2 Introduction to R We will go over some of the basic R operations in this chapter. If you have questions, you should check Chris Bilder’s website for full information. 2.1 Basic Operation 2+2 #&gt; [1] 4 2^3 #&gt; [1] 8 # calculate the cdf of std. normal pnorm(1.96) # 1.96 is the quantile #&gt; [1] 0.9750021 log(1) #&gt; [1] 0 sin(pi/2) #&gt; [1] 1 3/4 #&gt; [1] 0.75 save &lt;- 2+2 save #&gt; [1] 4 objects() #&gt; [1] &quot;save&quot; ls() #&gt; [1] &quot;save&quot; # quit operaiton # q() 2.2 Vectors x &lt;- c(1,2,3,4,5) x #&gt; [1] 1 2 3 4 5 sd(x) #&gt; [1] 1.581139 mysd &lt;- function(x){ cat(&quot; My data \\n&quot;, x, &quot;\\n has std deviation&quot;,sqrt(var(x))) } mysd(x) #&gt; My data #&gt; 1 2 3 4 5 #&gt; has std deviation 1.581139 pnorm(q=1.96, mean=1.96, sd=1) #&gt; [1] 0.5 The full syntax for pnorm() is pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) pnorm(q=c(-1.96,1.96)) #&gt; [1] 0.0249979 0.9750021 x &lt;- c(3.68, -3.63, 0.80, 3.03, -9.86, -8.66, -2.38, 8.94, 0.52, 1.25) y &lt;- c(0.55, 1.65, 0.98, -0.07, -0.01, -0.31, -0.34, -1.38, -1.32, 0.53) x+y #&gt; [1] 4.23 -1.98 1.78 2.96 -9.87 -8.97 -2.72 7.56 -0.80 #&gt; [10] 1.78 x*y #&gt; [1] 2.0240 -5.9895 0.7840 -0.2121 0.0986 2.6846 #&gt; [7] 0.8092 -12.3372 -0.6864 0.6625 mean(x) #&gt; [1] -0.631 x-mean(x) #&gt; [1] 4.311 -2.999 1.431 3.661 -9.229 -8.029 -1.749 9.571 #&gt; [9] 1.151 1.881 x*2 #&gt; [1] 7.36 -7.26 1.60 6.06 -19.72 -17.32 -4.76 17.88 #&gt; [9] 1.04 2.50 The element(elt)-wise operation makes our life easier. 2.3 Files Click gpa.csv to download the GPA csv file. Click gpa.txt to download the GPA txt file. getwd() #&gt; [1] &quot;/Users/weishangjie/Documents/GitHub/Time-Series-Analysis&quot; gpatxt &lt;- read.table(&quot;gpa.txt&quot;, header=TRUE, sep=&quot;&quot;) gpacsv &lt;- read.csv(&quot;gpa.csv&quot;) #write.table(x = gpacsv, file = &quot;gpa-out1.csv&quot;, quote = FALSE, row.names = # FALSE, sep =&quot;,&quot;) #write.csv(x = gpacsv, file = &quot;gpa-out2.csv&quot;) gpacsv$HSGPA #&gt; [1] 3.04 2.35 2.70 2.55 2.83 4.32 3.39 2.32 2.69 2.83 2.39 #&gt; [12] 3.65 2.85 3.83 2.22 1.98 2.88 4.00 2.28 2.88 gpacsv$CollegeGPA #&gt; [1] 3.10 2.30 3.00 2.45 2.50 3.70 3.40 2.60 2.80 3.60 2.00 #&gt; [12] 2.90 3.30 3.20 2.80 2.40 2.60 3.80 2.20 2.60 gpacsv[1,1] # [row, col] #&gt; [1] 3.04 gpacsv[,1] #&gt; [1] 3.04 2.35 2.70 2.55 2.83 4.32 3.39 2.32 2.69 2.83 2.39 #&gt; [12] 3.65 2.85 3.83 2.22 1.98 2.88 4.00 2.28 2.88 gpacsv[c(1,3,5),2] #&gt; [1] 3.1 3.0 2.5 gpacsv[,&quot;HSGPA&quot;] #&gt; [1] 3.04 2.35 2.70 2.55 2.83 4.32 3.39 2.32 2.69 2.83 2.39 #&gt; [12] 3.65 2.85 3.83 2.22 1.98 2.88 4.00 2.28 2.88 summary(gpacsv) #&gt; HSGPA CollegeGPA #&gt; Min. :1.980 Min. :2.000 #&gt; 1st Qu.:2.380 1st Qu.:2.487 #&gt; Median :2.830 Median :2.800 #&gt; Mean :2.899 Mean :2.862 #&gt; 3rd Qu.:3.127 3rd Qu.:3.225 #&gt; Max. :4.320 Max. :3.800 names(gpacsv) #&gt; [1] &quot;HSGPA&quot; &quot;CollegeGPA&quot; plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = &quot;College GPA vs. HS GPA&quot;, xlim = c(0,4.5), ylim = c(0,4.5), col = &quot;red&quot;, pch = 1, cex = 1.0, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) The plot() function creates a two dimensional plot of data. Here are descriptions of its arguments: x specifies what is plotted for the x-axis. y specifies what is plotted for the y-axis. xlab and ylab specify the x-axis and y-axis labels, respectively. main specifies the main title of the plot. xlim and ylim specify the x-axis and y-axis limits, respectively. Notice the use of the c() function. col specifies the color of the plotting points. Run the colors() function to see what possible colors can be used. Also, you can see Here for the colors from colors(). pch specifies the plotting characters. cexspecifies the height of the plotting characters. The value 1.0 is the default. panel.first = grid() specifies grid lines will be plotted. The line types can be specified as follows: 1=solid, 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash or as one of the character strings \"blank\", \"solid\", \"dashed\", \"dotted\", \"dotdash\", \"longdash\", or \"twodash\". These line type specifications can be used in other functions. The par()(parameter) function’s Help contains more information about the different plotting options! 2.4 Regression Our model is:\\[CollegeGPA=\\beta_0+\\beta_1HSGPA+\\epsilon\\] mod.fit &lt;- lm(formula= CollegeGPA~ HSGPA, data=gpacsv) mod.fit #&gt; #&gt; Call: #&gt; lm(formula = CollegeGPA ~ HSGPA, data = gpacsv) #&gt; #&gt; Coefficients: #&gt; (Intercept) HSGPA #&gt; 1.0869 0.6125 names(mod.fit) #&gt; [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; #&gt; [4] &quot;rank&quot; &quot;fitted.values&quot; &quot;assign&quot; #&gt; [7] &quot;qr&quot; &quot;df.residual&quot; &quot;xlevels&quot; #&gt; [10] &quot;call&quot; &quot;terms&quot; &quot;model&quot; mod.fit$coefficients #&gt; (Intercept) HSGPA #&gt; 1.0868795 0.6124941 round(mod.fit$residuals[1:5],2) #&gt; 1 2 3 4 5 #&gt; 0.15 -0.23 0.26 -0.20 -0.32 library(tidyverse) #&gt; ── Attaching packages ─────────────────── tidyverse 1.3.2 ── #&gt; ✔ ggplot2 3.4.1 ✔ purrr 1.0.1 #&gt; ✔ tibble 3.1.8 ✔ dplyr 1.1.0 #&gt; ✔ tidyr 1.3.0 ✔ stringr 1.5.0 #&gt; ✔ readr 2.1.4 ✔ forcats 0.5.2 #&gt; ── Conflicts ────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() save.fit &lt;- data.frame(gpacsv, C.GPA.hat = round(mod.fit$fitted.values,2), residuals = round(mod.fit$residuals,2)) save.fit %&gt;% head() #&gt; HSGPA CollegeGPA C.GPA.hat residuals #&gt; 1 3.04 3.10 2.95 0.15 #&gt; 2 2.35 2.30 2.53 -0.23 #&gt; 3 2.70 3.00 2.74 0.26 #&gt; 4 2.55 2.45 2.65 -0.20 #&gt; 5 2.83 2.50 2.82 -0.32 #&gt; 6 4.32 3.70 3.73 -0.03 summary(mod.fit) #&gt; #&gt; Call: #&gt; lm(formula = CollegeGPA ~ HSGPA, data = gpacsv) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.55074 -0.25086 0.01633 0.24242 0.77976 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.0869 0.3666 2.965 0.008299 ** #&gt; HSGPA 0.6125 0.1237 4.953 0.000103 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3437 on 18 degrees of freedom #&gt; Multiple R-squared: 0.5768, Adjusted R-squared: 0.5533 #&gt; F-statistic: 24.54 on 1 and 18 DF, p-value: 0.0001027 class(mod.fit) #&gt; [1] &quot;lm&quot; Hence, our estimated regression model is\\[ \\hat{collge.GPA}=\\hat{\\beta_0}+\\hat{\\beta_1}HS.GPA =1.0869+0.6125HS.GPA\\] # Open a new graphics window # device new dev.new(width = 8, height = 6, pointsize = 10) # 1 row and 2 columns of plots par(mfrow = c(1,2)) # par= graphic parameter # mfrow= make a frame by row # Same scatter plot as before plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = &quot;College GPA vs. HS GPA&quot;, xlim = c(0,4.5), ylim = c(0,4.5), col = &quot;red&quot;, pch = 1, cex = 1.0, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) # Puts the line y = a + bx on the plot abline(a = mod.fit$coefficients[1], b = mod.fit$coefficients[2], lty = &quot;solid&quot;, col = &quot;blue&quot;, lwd = 2) # Same scatter plot as before plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = &quot;College GPA vs. HS GPA&quot;, xlim = c(0,4.5), ylim = c(0,4.5), col = &quot;red&quot;, pch = 1, cex = 1.0, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) # Add line # expr= math expression curve(expr = mod.fit$coefficients[1] + mod.fit$coefficients[2]*x, xlim = c(min(gpacsv$HSGPA),max(gpacsv$HSGPA)), col= &quot;blue&quot;, add = TRUE, lwd = 2) # Draw a line from (x0, y0) to (x1, y1) segments(x0 = min(gpacsv$HSGPA), y0 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*min(gpacsv$HSGPA), x1 = max(gpacsv$HSGPA), y1 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*max(gpacsv$HSGPA), lty = 1, col = &quot;blue&quot;, lwd = 2) The dev.new() function can be used to open a new plotting window. The abline() function can be used to draw straight lines on a plot. In the format used here, the line y = a + bx was drawn where a was the (intercept) and b was the (slope). In the second plot, the curve() function was used to draw the line on the plot. This was done to have the line within the range of the high school GPA values. Let’s use function to automate what we have done. my.reg.func &lt;- function(x, y, data) { # Fit the simple linear regression model and save the results in mod.fit mod.fit &lt;- lm(formula = y ~ x, data = data) #Open a new graphics window - do not need to dev.new(width = 6, height = 6, pointsize = 10) # Same scatter plot as before plot(x = x, y = y, xlab = &quot;x&quot;, ylab = &quot;y&quot;, main = &quot;y vs. x&quot;, panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) # Plot model curve(expr = mod.fit$coefficients[1] + mod.fit$coefficients[2]*x, xlim = c(min(x),max(x)), col = &quot;blue&quot;, add = TRUE) segments(x0 = min(x), y0 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*min(x), x1 = max(x), y1 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*max(x), lty = 1, col = &quot;blue&quot;, lwd = 2) # This is the object returned mod.fit } save.it &lt;- my.reg.func(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, data = gpacsv) To get specific x-axis or y-axis tick marks on a plot, use the axis() function. For example, #Note that xaxt = &quot;n&quot; tells R to not give any labels on the # x-axis (yaxt = &quot;n&quot; works for y-axis) plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = &quot;College GPA vs. HS GPA&quot;, xaxt = &quot;n&quot;, xlim = c(0, 4.5), ylim = c(0, 4.5), col = &quot;red&quot;, pch = 1) plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = &quot;College GPA vs. HS GPA&quot;, xaxt = &quot;n&quot;, xlim = c(0, 4.5), ylim = c(0, 4.5), col = &quot;red&quot;, pch = 1) #Major tick marks axis(side = 1, at = seq(from = 0, to = 4.5, by = 0.5)) plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = &quot;College GPA vs. HS GPA&quot;, xaxt = &quot;n&quot;, xlim = c(0, 4.5), ylim = c(0, 4.5), col = &quot;red&quot;, pch = 1) #Major tick marks axis(side = 1, at = seq(from = 0, to = 4.5, by = 0.5)) #Minor tick marks axis(side = 1, at = seq(from = 0, to = 4.5, by = 0.1), tck = 0.01, labels = FALSE) plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = expression(hat(Y) == hat(beta)[0] + hat(beta)[1]*x), xlim = c(0,4.5), ylim = c(0,4.5), col = &quot;red&quot;, pch = 1, cex = 1.0, panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) #Draw a line from (x0, y0) to (x1, y1) segments(x0 = min(gpacsv$HSGPA), y0 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*min(gpacsv$HSGPA), x1 = max(gpacsv$HSGPA), y1 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*max(gpacsv$HSGPA), lty = 1, col = &quot;blue&quot;, lwd = 2) plot(x = gpacsv$HSGPA, y = gpacsv$CollegeGPA, xlab = &quot;HS GPA&quot;, ylab = &quot;College GPA&quot;, main = expression(paste(&quot;College GPA vs. HS GPA and &quot;, widehat(CollegeGPA) == hat(beta)[0] + hat(beta)[1]*HSGPA)), xlim = c(0,4.5), ylim = c(0,4.5), col = &quot;red&quot;, pch = 1, cex = 1.0, panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) #Draw a line from (x0, y0) to (x1, y1) segments(x0 = min(gpacsv$HSGPA), y0 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*min(gpacsv$HSGPA), x1 = max(gpacsv$HSGPA), y1 = mod.fit$coefficients[1] + mod.fit$coefficients[2]*max(gpacsv$HSGPA), lty = 1, col = &quot;blue&quot;, lwd = 2) demo(plotmath) #Run this to see examples #&gt; #&gt; #&gt; demo(plotmath) #&gt; ---- ~~~~~~~~ #&gt; #&gt; &gt; # Copyright (C) 2002-2016 The R Core Team #&gt; &gt; #&gt; &gt; require(datasets) #&gt; #&gt; &gt; require(grDevices); require(graphics) #&gt; #&gt; &gt; ## --- &quot;math annotation&quot; in plots : #&gt; &gt; #&gt; &gt; ###### #&gt; &gt; # create tables of mathematical annotation functionality #&gt; &gt; ###### #&gt; &gt; make.table &lt;- function(nr, nc) { #&gt; + savepar &lt;- par(mar=rep(0, 4), pty=&quot;s&quot;) #&gt; + plot(c(0, nc*2 + 1), c(0, -(nr + 1)), #&gt; + type=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, axes=FALSE) #&gt; + savepar #&gt; + } #&gt; #&gt; &gt; get.r &lt;- function(i, nr) { #&gt; + i %% nr + 1 #&gt; + } #&gt; #&gt; &gt; get.c &lt;- function(i, nr) { #&gt; + i %/% nr + 1 #&gt; + } #&gt; #&gt; &gt; draw.title.cell &lt;- function(title, i, nr) { #&gt; + r &lt;- get.r(i, nr) #&gt; + c &lt;- get.c(i, nr) #&gt; + text(2*c - .5, -r, title) #&gt; + rect((2*(c - 1) + .5), -(r - .5), (2*c + .5), -(r + .5)) #&gt; + } #&gt; #&gt; &gt; draw.plotmath.cell &lt;- function(expr, i, nr, string = NULL) { #&gt; + r &lt;- get.r(i, nr) #&gt; + c &lt;- get.c(i, nr) #&gt; + if (is.null(string)) { #&gt; + string &lt;- deparse(expr) #&gt; + string &lt;- substr(string, 12, nchar(string) - 1) #&gt; + } #&gt; + text((2*(c - 1) + 1), -r, string, col=&quot;grey50&quot;) #&gt; + text((2*c), -r, expr, adj=c(.5,.5)) #&gt; + rect((2*(c - 1) + .5), -(r - .5), (2*c + .5), -(r + .5), border=&quot;grey&quot;) #&gt; + } #&gt; #&gt; &gt; nr &lt;- 20 #&gt; #&gt; &gt; nc &lt;- 2 #&gt; #&gt; &gt; oldpar &lt;- make.table(nr, nc) #&gt; #&gt; &gt; i &lt;- 0 #&gt; #&gt; &gt; draw.title.cell(&quot;Arithmetic Operators&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x + y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x - y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x * y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x / y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %+-% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %/% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %*% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %.% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(-x), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(+x), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Sub/Superscripts&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x[i]), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x^2), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Juxtaposition&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x * y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(paste(x, y, z)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Radicals&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(sqrt(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(sqrt(x, y)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Lists&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(list(x, y, z)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Relations&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x == y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x != y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x &lt; y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x &lt;= y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x &gt; y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x &gt;= y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %~~% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %=~% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %==% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %prop% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %~% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Typeface&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(plain(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(italic(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(bold(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(bolditalic(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(underline(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; # Need fewer, wider columns for ellipsis ... #&gt; &gt; nr &lt;- 20 #&gt; #&gt; &gt; nc &lt;- 2 #&gt; #&gt; &gt; make.table(nr, nc) #&gt; $mar #&gt; [1] 0 0 0 0 #&gt; #&gt; $pty #&gt; [1] &quot;s&quot; #&gt; #&gt; #&gt; &gt; i &lt;- 0 #&gt; #&gt; &gt; draw.title.cell(&quot;Ellipsis&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(list(x[1], ..., x[n])), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x[1] + ... + x[n]), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(list(x[1], cdots, x[n])), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x[1] + ldots + x[n]), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Set Relations&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %subset% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %subseteq% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %supset% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %supseteq% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %notsubset% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %in% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %notin% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Accents&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(hat(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(tilde(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(ring(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(bar(xy)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(widehat(xy)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(widetilde(xy)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Arrows&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %&lt;-&gt;% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %-&gt;% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %&lt;-% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %up% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %down% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %&lt;=&gt;% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %=&gt;% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %&lt;=% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %dblup% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x %dbldown% y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Symbolic Names&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(Alpha - Omega), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(alpha - omega), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(phi1 + sigma1), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(Upsilon1), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(infinity), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(32 * degree), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(60 * minute), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(30 * second), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; # Need even fewer, wider columns for typeface and style ... #&gt; &gt; nr &lt;- 20 #&gt; #&gt; &gt; nc &lt;- 1 #&gt; #&gt; &gt; make.table(nr, nc) #&gt; $mar #&gt; [1] 0 0 0 0 #&gt; #&gt; $pty #&gt; [1] &quot;s&quot; #&gt; #&gt; #&gt; &gt; i &lt;- 0 #&gt; #&gt; &gt; draw.title.cell(&quot;Style&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(displaystyle(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(textstyle(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(scriptstyle(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(scriptscriptstyle(x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Spacing&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x ~~ y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; # Need fewer, taller rows for fractions ... #&gt; &gt; # cheat a bit to save pages #&gt; &gt; par(new = TRUE) #&gt; #&gt; &gt; nr &lt;- 10 #&gt; #&gt; &gt; nc &lt;- 1 #&gt; #&gt; &gt; make.table(nr, nc) #&gt; $mar #&gt; [1] 0 0 0 0 #&gt; #&gt; $pty #&gt; [1] &quot;s&quot; #&gt; #&gt; #&gt; &gt; i &lt;- 4 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x + phantom(0) + y), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x + over(1, phantom(0))), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.title.cell(&quot;Fractions&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(frac(x, y)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(over(x, y)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(atop(x, y)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; # Need fewer, taller rows and fewer, wider columns for big operators ... #&gt; &gt; nr &lt;- 10 #&gt; #&gt; &gt; nc &lt;- 1 #&gt; #&gt; &gt; make.table(nr, nc) #&gt; $mar #&gt; [1] 0 0 0 0 #&gt; #&gt; $pty #&gt; [1] &quot;s&quot; #&gt; #&gt; #&gt; &gt; i &lt;- 0 #&gt; #&gt; &gt; draw.title.cell(&quot;Big Operators&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(sum(x[i], i=1, n)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(prod(plain(P)(X == x), x)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(integral(f(x) * dx, a, b)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(union(A[i], i==1, n)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(intersect(A[i], i==1, n)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(lim(f(x), x %-&gt;% 0)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(min(g(x), x &gt;= 0)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(inf(S)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(sup(S)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; nr &lt;- 12 #&gt; #&gt; &gt; nc &lt;- 1 #&gt; #&gt; &gt; make.table(nr, nc) #&gt; $mar #&gt; [1] 0 0 0 0 #&gt; #&gt; $pty #&gt; [1] &quot;s&quot; #&gt; #&gt; #&gt; &gt; i &lt;- 0 #&gt; #&gt; &gt; draw.title.cell(&quot;Grouping&quot;, i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; # Those involving &#39;{ . }&#39; have to be done &quot;by hand&quot; #&gt; &gt; draw.plotmath.cell(expression({}(x , y)), i, nr, string=&quot;{}(x, y)&quot;); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression((x + y)*z), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x^y + z), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x^(y + z)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(x^{y + z}), i, nr, string=&quot;x^{y + z}&quot;); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(group(&quot;(&quot;, list(a, b), &quot;]&quot;)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(bgroup(&quot;(&quot;, atop(x, y), &quot;)&quot;)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(group(lceil, x, rceil)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(group(lfloor, x, rfloor)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(group(langle, list(x, y), rangle)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; draw.plotmath.cell(expression(group(&quot;|&quot;, x, &quot;|&quot;)), i, nr); i &lt;- i + 1 #&gt; #&gt; &gt; par(oldpar) 2.5 Object-Oriented Language Functions are typically designed to operate on only one or very few classes of objects. However, some functions, like summary(), are generic, in the sense that essentially different versions of them have been constructed to work with different classes of objects. When a generic function is run with an object, R first checks the object’s class type and then looks to find a method function with the name format &lt;generic function&gt;.&lt;class name&gt;. Below are examples for summary(): summary(mod.fit) – The function summary.lm() summarizes the regression model summary(gpacsv) – The function summary.data.frame() summarizes the data frame’s contents summary.default() – R attempts to run this function if there is no method function for a class There are many generic functions! For example, plot() is a generic function (tryplot(mod.fit) to see what happens!). We will also see other generic functions like predict() later in the notes. plot(mod.fit) The purpose of generic functions is to use a familiar language set with any object. So it is convenient to use the same language set no matter the application. This is why R is referred to as an object-oriented language. To see a list of all method functions associated with a class, use methods(class = &lt;class name&gt;). For the regression example, the method functions associated with the lm class are: methods(class=&quot;lm&quot;) %&gt;% head() #&gt; [1] &quot;add1.lm&quot; &quot;alias.lm&quot; #&gt; [3] &quot;anova.lm&quot; &quot;case.names.lm&quot; #&gt; [5] &quot;coerce,oldClass,S3-method&quot; &quot;confint.lm&quot; To see a list of all method functions for a generic function, use methods(generic.function = &lt;generic function name&gt;) methods(generic.function = &quot;summary&quot;) %&gt;% head() #&gt; [1] &quot;summary,ANY-method&quot; #&gt; [2] &quot;summary,DBIObject-method&quot; #&gt; [3] &quot;summary.aov&quot; #&gt; [4] &quot;summary.aovlist&quot; #&gt; [5] &quot;summary.aspell&quot; #&gt; [6] &quot;summary.check_packages_in_dir&quot; Knowing what a name of a particular method function can be helpful to find help on it. For example, the help for summary() alone is not very helpful! However, the help for summary.lm()provides a lot of useful information about what is summarized for a regression model. "],["plotting.html", "Chapter 3 Plotting 3.1 Example Data 3.2 S&amp;P500 Index 3.3 Sunspots", " Chapter 3 Plotting In this chapter, we will go over some Time Series examples. The aim of this chapter is to help you grasp some of the ideas about plotting. 3.1 Example Data Click OSU_enroll.csv to download data. osu.enroll &lt;- read.csv(file = &quot;OSU_enroll.csv&quot;, stringsAsFactors = TRUE) head(osu.enroll) #&gt; t Semester Year Enrollment date #&gt; 1 1 Fall 1989 20110 8/31/1989 #&gt; 2 2 Spring 1990 19128 2/1/1990 #&gt; 3 3 Summer 1990 7553 6/1/1990 #&gt; 4 4 Fall 1990 19591 8/31/1990 #&gt; 5 5 Spring 1991 18361 2/1/1991 #&gt; 6 6 Summer 1991 6702 6/1/1991 tail(osu.enroll) #&gt; t Semester Year Enrollment date #&gt; 35 35 Spring 2001 20004 2/1/2001 #&gt; 36 36 Summer 2001 7558 6/1/2001 #&gt; 37 37 Fall 2001 21872 8/31/2001 #&gt; 38 38 Spring 2002 20922 2/1/2002 #&gt; 39 39 Summer 2002 7868 6/1/2002 #&gt; 40 40 Fall 2002 22992 8/31/2002 x &lt;- osu.enroll$Enrollment #One way to do plot dev.new(width = 8, height = 6, pointsize = 10) # we did not specify y-axis and R put our x in y-axis, time in x-axis plot(x = x, ylab = &quot;OSU Enrollment&quot;, xlab = &quot;t (time)&quot;, type=&quot;l&quot;, col = &quot;red&quot;, main = &quot;OSU Enrollment from Fall 1989 to Fall 2002&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) # A little different version of the plot plot(x = x, ylab = &quot;OSU Enrollment&quot;, type = &quot;o&quot;, xlab = &quot;t (time)&quot;, col = &quot;red&quot;, main = &quot;OSU enrollment data&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) dev.new(width = 8, height = 6, pointsize = 10) # we did not specify y-axis and R put our x in y-axis, time in x-axis plot(x = x, ylab = &quot;OSU Enrollment&quot;, xlab = &quot;t (time)&quot;, type=&quot;l&quot;, col = &quot;red&quot;, main = &quot;OSU Enrollment from Fall 1989 to Fall 2002&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = osu.enroll$Enrollment, pch = 20, col = &quot;blue&quot;) Altenatively, you can do the same thing using ggplot. library(ggplot2) # Create a data frame df &lt;- data.frame(osu.enroll) # Create the plot ggplot(df, aes(x = t, y = Enrollment)) + geom_line(colour = &quot;red&quot;) + # Line plot geom_point(shape = 20, colour = &quot;blue&quot;) + # Add points labs(x = &quot;t (time)&quot;, y = &quot;OSU Enrollment&quot;, title = &quot;OSU Enrollment from Fall 1989 to Fall 2002&quot;) + # Set axis labels and title theme_bw() + # Set the theme to a white background with black lines theme(panel.grid.major = element_line(colour = &quot;gray&quot;, linetype = &quot;dotted&quot;)) # Add gray dotted lines to the plot When only x is specified in the plot() function, R puts this on the y-axis and uses the observation number on the x-axis. Compare this to the next plot below where both x and y arguments are specified. #More complicated plot fall &lt;- osu.enroll[osu.enroll$Semester == &quot;Fall&quot;,] spring &lt;- osu.enroll[osu.enroll$Semester == &quot;Spring&quot;,] summer &lt;- osu.enroll[osu.enroll$Semester == &quot;Summer&quot;,] plot(y = fall$Enrollment, x = fall$t, ylab = &quot;OSU Enrollment&quot;, xlab = &quot;t (time)&quot;, col = &quot;blue&quot;, main = &quot;OSU Enrollment from Fall 1989 to Fall 2002&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;), pch = 1, type = &quot;o&quot;, ylim = c(0,max(osu.enroll$Enrollment))) lines(y = spring$Enrollment, x = spring$t, col = &quot;red&quot;, type = &quot;o&quot;, pch = 2) lines(y = summer$Enrollment, x = summer$t, col = &quot;darkgreen&quot;, type = &quot;o&quot;, pch = 3) legend(x=&quot;center&quot;, legend= c(&quot;Fall&quot;,&quot;Spring&quot;,&quot;Summer&quot;), pch=c(1,2,3), lty=c(1,1,1), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;darkgreen&quot;), bty=&quot;n&quot;) #Another way to do plot with actual dates plot(y = osu.enroll$Enrollment, x = as.Date(osu.enroll$date, format = &quot;%m/%d/%Y&quot;), xlab = &quot;Time&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;OSU Enrollment from Fall 1989 to Fall 2002&quot;, ylab = &quot;OSU Enrollment&quot;) points(y = osu.enroll$Enrollment, x = as.Date(osu.enroll$date, format = &quot;%m/%d/%Y&quot;), pch = 20, col = &quot;blue&quot;) #Create own gridlines # v specifies vertical line; h specifies horizontal line abline(v = as.Date(c(&quot;1990/1/1&quot;, &quot;1992/1/1&quot;, &quot;1994/1/1&quot;, &quot;1996/1/1&quot;, &quot;1998/1/1&quot;, &quot;2000/1/1&quot;, &quot;2002/1/1&quot;)), lty = &quot;dotted&quot;, col = &quot;lightgray&quot;) abline(h = c(10000, 15000, 20000), lty = &quot;dotted&quot;, col = &quot;lightgray&quot;) # Autocorrelation rho.x &lt;- acf(x = x, type = &quot;correlation&quot;, main = &quot;OSU Enrollment series&quot;) rho.x #&gt; #&gt; Autocorrelations of series &#39;x&#39;, by lag #&gt; #&gt; 0 1 2 3 4 5 6 7 #&gt; 1.000 -0.470 -0.425 0.909 -0.438 -0.395 0.822 -0.403 #&gt; 8 9 10 11 12 13 14 15 #&gt; -0.358 0.739 -0.367 -0.327 0.655 -0.337 -0.297 0.581 #&gt; 16 #&gt; -0.309 rho.x$acf[1:9] #&gt; [1] 1.0000000 -0.4702315 -0.4253427 0.9087421 -0.4377336 #&gt; [6] -0.3946048 0.8224660 -0.4025871 -0.3584216 3.2 S&amp;P500 Index Click SP500weekly.csv to download data. SP500 &lt;- read.csv(file=&quot;SP500weekly.csv&quot;,stringsAsFactors = TRUE) head(SP500) #&gt; WeekStart Open High Low Close AdjClose Volume #&gt; 1 1/1/1995 459.21 462.49 457.20 460.68 460.68 1199080000 #&gt; 2 1/8/1995 460.67 466.43 458.65 465.97 465.97 1627330000 #&gt; 3 1/15/1995 465.97 470.43 463.99 464.78 464.78 1667400000 #&gt; 4 1/22/1995 464.78 471.36 461.14 470.39 470.39 1628110000 #&gt; 5 1/29/1995 470.39 479.91 467.49 478.65 478.65 1888560000 #&gt; 6 2/5/1995 478.64 482.60 478.36 481.46 481.46 1579920000 tail(SP500) #&gt; WeekStart Open High Low Close AdjClose #&gt; 1395 9/19/2021 4402.95 4465.40 4305.91 4455.48 4455.48 #&gt; 1396 9/26/2021 4442.12 4457.30 4288.52 4357.04 4357.04 #&gt; 1397 10/3/2021 4348.84 4429.97 4278.94 4391.34 4391.34 #&gt; 1398 10/10/2021 4385.44 4475.82 4329.92 4471.37 4471.37 #&gt; 1399 10/17/2021 4463.72 4559.67 4447.47 4544.90 4544.90 #&gt; 1400 10/24/2021 4553.69 4608.08 4537.36 4605.38 4605.38 #&gt; Volume #&gt; 1395 15697030000 #&gt; 1396 15555390000 #&gt; 1397 14795520000 #&gt; 1398 13758090000 #&gt; 1399 13966070000 #&gt; 1400 16206040000 x &lt;- SP500$Close #One way to do plot dev.new(width = 8, height = 6, pointsize = 10) #again, we do not specify y-axis here plot(x = x, ylab = &quot;S&amp;P 500 Index&quot;, xlab = &quot;t (time)&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;S&amp;P 500 Index from 1/1/1995 to 10/25/2021 (weekly)&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) #Another way to do plot with actual dates plot(y = x, x = as.Date(SP500$WeekStart, format = &quot;%m/%d/%Y&quot;), xlab = &quot;Time&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;S&amp;P 500 Index from 1/1/1995 to 10/25/2021 (weekly)&quot;, ylab = &quot;S&amp;P 500 Index&quot;) #Create own gridlines abline(v = as.Date(c(&quot;1995/1/1&quot;, &quot;2000/1/1&quot;, &quot;2005/1/1&quot;, &quot;2010/1/1&quot;, &quot;2015/1/1&quot;, &quot;2020/1/1&quot;)), lty = &quot;dotted&quot;, col = &quot;lightgray&quot;) abline(h = seq(from = 0, to = 5000, by = 1000), lty = &quot;dotted&quot;, col = &quot;lightgray&quot;) # One more way with fine control of the dates plot(y = x, x = as.Date(SP500$WeekStart, format = &quot;%m/%d/%Y&quot;), xlab = &quot;Time&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;S&amp;P 500 Index from 1/1/1995 to 10/25/2021 (weekly)&quot;, ylab = &quot;S&amp;P 500 Index&quot;, xaxt = &quot;n&quot;) axis.Date(side = 1, at = seq(from = as.Date(&quot;1995/1/1&quot;), to = as.Date(&quot;2021/12/31&quot;), by = &quot;years&quot;), labels = format(x = seq(from = as.Date(&quot;1995/1/1&quot;), to = as.Date(&quot;2021/12/31&quot;), by = &quot;years&quot;), format = &quot;%b%y&quot;), las = 2) #las changes orientation of labels #Create own gridlines abline(v = as.Date(c(&quot;1995/1/1&quot;, &quot;2000/1/1&quot;, &quot;2005/1/1&quot;, &quot;2010/1/1&quot;, &quot;2015/1/1&quot;, &quot;2020/1/1&quot;)), lty = &quot;dotted&quot;, col = &quot;lightgray&quot;) abline(h = seq(from = 0, to = 5000, by = 1000), lty = &quot;dotted&quot;, col = &quot;lightgray&quot;) 3.3 Sunspots Click SN_y_tot_V2.0.csv to download data. sunspots &lt;- read.table(file = &quot;SN_y_tot_V2.0.csv&quot;, sep = &quot;;&quot;, col.names = c(&quot;Mid.year&quot;, &quot;Mean.total&quot;, &quot;Mean.SD.total&quot;, &quot;Numb.obs.used&quot;, &quot;Definitive&quot;)) head(sunspots) #&gt; Mid.year Mean.total Mean.SD.total Numb.obs.used #&gt; 1 1700.5 8.3 -1 -1 #&gt; 2 1701.5 18.3 -1 -1 #&gt; 3 1702.5 26.7 -1 -1 #&gt; 4 1703.5 38.3 -1 -1 #&gt; 5 1704.5 60.0 -1 -1 #&gt; 6 1705.5 96.7 -1 -1 #&gt; Definitive #&gt; 1 1 #&gt; 2 1 #&gt; 3 1 #&gt; 4 1 #&gt; 5 1 #&gt; 6 1 tail(sunspots) #&gt; Mid.year Mean.total Mean.SD.total Numb.obs.used #&gt; 316 2015.5 69.8 6.4 8903 #&gt; 317 2016.5 39.8 3.9 9940 #&gt; 318 2017.5 21.7 2.5 11444 #&gt; 319 2018.5 7.0 1.1 12611 #&gt; 320 2019.5 3.6 0.5 12884 #&gt; 321 2020.5 8.8 4.1 14440 #&gt; Definitive #&gt; 316 1 #&gt; 317 1 #&gt; 318 1 #&gt; 319 1 #&gt; 320 1 #&gt; 321 1 dev.new(width = 8, height = 6, pointsize = 10) #again, we did not specify y-axis here plot(x = sunspots$Mean.total, ylab = &quot;Number of sunspots&quot;, xlab = &quot;t (time)&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;Sunspots per year from 1700 to 2020&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = sunspots$Mean.total, pch = 20, col = &quot;blue&quot;) # Include dates plot(y = sunspots$Mean.total, x = sunspots$Mid.year, ylab = &quot;Number of sunspots&quot;, xlab = &quot;Year&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = &quot;Sunspots per year from 1700 to 2020&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(y = sunspots$Mean.total, x = sunspots$Mid.year, pch = 20, col = &quot;blue&quot;) #Convert to an object of class &quot;ts&quot; x &lt;- ts(data = sunspots$Mean.total, start = 1700, frequency = 1) x #&gt; Time Series: #&gt; Start = 1700 #&gt; End = 2020 #&gt; Frequency = 1 #&gt; [1] 8.3 18.3 26.7 38.3 60.0 96.7 48.3 33.3 16.7 #&gt; [10] 13.3 5.0 0.0 0.0 3.3 18.3 45.0 78.3 105.0 #&gt; [19] 100.0 65.0 46.7 43.3 36.7 18.3 35.0 66.7 130.0 #&gt; [28] 203.3 171.7 121.7 78.3 58.3 18.3 8.3 26.7 56.7 #&gt; [37] 116.7 135.0 185.0 168.3 121.7 66.7 33.3 26.7 8.3 #&gt; [46] 18.3 36.7 66.7 100.0 134.8 139.0 79.5 79.7 51.2 #&gt; [55] 20.3 16.0 17.0 54.0 79.3 90.0 104.8 143.2 102.0 #&gt; [64] 75.2 60.7 34.8 19.0 63.0 116.3 176.8 168.0 136.0 #&gt; [73] 110.8 58.0 51.0 11.7 33.0 154.2 257.3 209.8 141.3 #&gt; [82] 113.5 64.2 38.0 17.0 40.2 138.2 220.0 218.2 196.8 #&gt; [91] 149.8 111.0 100.0 78.2 68.3 35.5 26.7 10.7 6.8 #&gt; [100] 11.3 24.2 56.7 75.0 71.8 79.2 70.3 46.8 16.8 #&gt; [109] 13.5 4.2 0.0 2.3 8.3 20.3 23.2 59.0 76.3 #&gt; [118] 68.3 52.9 38.5 24.2 9.2 6.3 2.2 11.4 28.2 #&gt; [127] 59.9 83.0 108.5 115.2 117.4 80.8 44.3 13.4 19.5 #&gt; [136] 85.8 192.7 227.3 168.7 143.0 105.5 63.3 40.3 18.1 #&gt; [145] 25.1 65.8 102.7 166.3 208.3 182.5 126.3 122.0 102.7 #&gt; [154] 74.1 39.0 12.7 8.2 43.4 104.4 178.3 182.2 146.6 #&gt; [163] 112.1 83.5 89.2 57.8 30.7 13.9 62.8 123.6 232.0 #&gt; [172] 185.3 169.2 110.1 74.5 28.3 18.9 20.7 5.7 10.0 #&gt; [181] 53.7 90.5 99.0 106.1 105.8 86.3 42.4 21.8 11.2 #&gt; [190] 10.4 11.8 59.5 121.7 142.0 130.0 106.6 69.4 43.8 #&gt; [199] 44.4 20.2 15.7 4.6 8.5 40.8 70.1 105.5 90.1 #&gt; [208] 102.8 80.9 73.2 30.9 9.5 6.0 2.4 16.1 79.0 #&gt; [217] 95.0 173.6 134.6 105.7 62.7 43.5 23.7 9.7 27.9 #&gt; [226] 74.0 106.5 114.7 129.7 108.2 59.4 35.1 18.6 9.2 #&gt; [235] 14.6 60.2 132.8 190.6 182.6 148.0 113.0 79.2 50.8 #&gt; [244] 27.1 16.1 55.3 154.3 214.7 193.0 190.7 118.9 98.3 #&gt; [253] 45.0 20.1 6.6 54.2 200.7 269.3 261.7 225.1 159.0 #&gt; [262] 76.4 53.4 39.9 15.0 22.0 66.8 132.9 150.0 149.4 #&gt; [271] 148.0 94.4 97.6 54.1 49.2 22.5 18.4 39.3 131.0 #&gt; [280] 220.1 218.9 198.9 162.4 91.0 60.5 20.6 14.8 33.9 #&gt; [289] 123.0 211.1 191.8 203.3 133.0 76.1 44.9 25.1 11.6 #&gt; [298] 28.9 88.3 136.3 173.9 170.4 163.6 99.3 65.3 45.8 #&gt; [307] 24.7 12.6 4.2 4.8 24.9 80.8 84.5 94.0 113.3 #&gt; [316] 69.8 39.8 21.7 7.0 3.6 8.8 class(x) #&gt; [1] &quot;ts&quot; class(sunspots$Mean.total) #&gt; [1] &quot;numeric&quot; 3.3.1 plot.ts() plot() is a generic function - uses the plot.ts() method function # we did not specify y-axis here, but x is now ts plot(x = x, ylab = expression(paste(x[t], &quot; (Number of sunspots)&quot;)), xlab = &quot;Year&quot;, type = &quot;o&quot;, col = &quot;red&quot;, main = &quot;Sunspots per year from 1700 to 2020&quot;) plot.ts(x = x, ylab = expression(paste(x[t], &quot; (Number of sunspots)&quot;)), xlab = &quot;Year&quot;, type = &quot;o&quot;, col = &quot;red&quot;, main = &quot;Sunspots per year from 1700 to 2020&quot;) #type = &quot;b&quot; also works for &quot;both&quot; points and lines, but it leaves spaces between the points and lines "],["basic-model.html", "Chapter 4 Basic Model 4.1 White Noise 4.2 Moving Average 4.3 Autoregression", " Chapter 4 Basic Model In this chapter, we will go introduce some basic Time Series model. Hopefully, we can discuss them in details in the following chapters. Definition 4.1 Stochastic process Stochastic process is a collection of random variables \\(\\{X_t\\}\\) indexed by t. Time Series is a collection of random vatiables indexed according to the order they are obtained in time. A realization of the stochastic process is the observed values. 4.1 White Noise Example 4.1 White Noise \\(W_t\\sim \\mathrm{i.i.d.} (0,\\sigma^2) , \\forall t=1,...,n\\) usually, we assume normal distribution, i.e., \\(W_t\\sim \\mathrm{i.i.d.} N(0,\\sigma^2) , \\forall t=1,...,n\\) set.seed(8128) w &lt;- rnorm(n = 100, mean = 0, sd = 1) head(w) #&gt; [1] -0.10528941 0.25548490 0.82065388 0.04070997 #&gt; [5] -0.66722880 -1.54502793 dev.new(width = 6, height = 6, pointsize = 10) # we did not specify y-axis here # note that we use expression() to type math expression plot(x = w, ylab = expression(w[t]), xlab = &quot;t&quot;, type = &quot;o&quot;, col = &quot;red&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot; ~ ind. N(0, 1)&quot;)), panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) #&gt; Warning in title(...): font metrics unknown for character #&gt; 0xa #&gt; Warning in title(...): font metrics unknown for character #&gt; 0xa #Advantage of second plot is separate control over color of points plot(x = w, ylab = expression(w[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot; ~ ind. N(0, 1)&quot;)), panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = w, pch = 20, col = &quot;blue&quot;) Suppose another white noise process is simulated. Below is a plot overlaying the two time series. set.seed(1298) w.new &lt;- rnorm(n = 100, mean = 0, sd = 1) head(w.new) #&gt; [1] 1.08820292 -1.46217413 -1.10887422 0.55156914 #&gt; [5] 0.70582813 0.05079594 par(mfrow=c(1,1)) plot(x = w, ylab = expression(w[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot; ~ ind. N(0, 1)&quot;)), panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = w, pch = 20, col = &quot;blue&quot;) lines(x = w.new, col = &quot;green&quot;) points(x = w.new, pch = 20,col = &quot;orange&quot;) legend(x =&quot;top&quot;,legend=c(&quot;Time series 1&quot;, &quot;Time series 2&quot;), lty=c(1,1), col=c(&quot;red&quot;, &quot;green&quot;), bty=&quot;n&quot;) We could also plot the two time series separately. dev.new(width = 8, height = 6, pointsize = 10) #Open a new plot window #make frame by row 2 rows 1 cols par(mfrow = c(2,1)) plot(x = w, ylab = expression(w[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot;~N(0, 1)&quot;)), panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = w, pch = 20, col = &quot;blue&quot;) plot(x = w.new, ylab = expression(w.new[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;green&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot; ~ ind.N(0, 1)&quot;)), panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = w.new, pch = 20, col = &quot;orange&quot;) # What if used plot.ts()? dev.new(width = 8, height = 6, pointsize = 10) #Open a new plot window plot.ts(x = cbind(w, w.new), ylab = expression(w[t]), xlab = &quot;t&quot;, type = &quot;o&quot;, col = &quot;red&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot; ~ ind. N(0, 1)&quot;)), panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) #Problem: gridlines do not extend to second plot plot.ts(x = cbind(w, w.new), ylab = expression(w[t]), xlab = &quot;t&quot;, type = &quot;o&quot;, col = &quot;red&quot;, main = expression(paste(&quot;White noise where &quot;, w[t], &quot; ~ ind. N(0, 1)&quot;))) grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;) #Problem: gridlines do not appear correctly on plots (could fix by specifying where to draw them using abline) 4.2 Moving Average Example 4.2 Moving Average of White Noise The previous time series had no correlation between the observations. One way to induce correlation is to create a “moving average” of the observations. This will have an effect of “smoothing” the series. Let \\(m_t = \\frac{w_t+w_{t-1}+w_{t-2}}{3}\\). This can be done in R using the following code: set.seed(8128) w &lt;- rnorm(n=100,mean=0, sd=1) head(w) #&gt; [1] -0.10528941 0.25548490 0.82065388 0.04070997 #&gt; [5] -0.66722880 -1.54502793 dev.new(width = 8, height = 6, pointsize = 10) #Open a new plot window plot(x = w, ylab = expression(paste(m[t], &quot; or &quot;, w[t])), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;), lty = &quot;dotted&quot;) points(x = w, pch = 20, col = &quot;blue&quot;) # rep(1/3,3) repeats 1/3 3 times # we can use filter() to generate moveing average # filter() help us build linear combination of elts of w # filter= linear combination coef # convolution for moving average # sides=1，表示只使用輸入向量的左側。 m &lt;- filter(w, filter = rep(1/3, 3), method=&quot;convolution&quot;, sides=1) head(m) #&gt; [1] NA NA 0.32361646 0.37228292 #&gt; [5] 0.06471168 -0.72384892 tail(m) #&gt; [1] 0.3158762 -0.1803096 0.2598066 -0.6450531 -0.5879723 #&gt; [6] -0.9120182 (w[1]+w[2]+w[3])/3 #&gt; [1] 0.3236165 (w[98]+w[99]+w[100])/3 #&gt; [1] -0.9120182 plot(x = w, ylab = expression(paste(m[t], &quot; or &quot;, w[t])), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;), lty = &quot;dotted&quot;) points(x = w, pch = 20, col = &quot;blue&quot;) lines(x = m, col = &quot;brown&quot;, lty = &quot;solid&quot;, lwd = 4) points(x = m, pch = 20, col = &quot;orange&quot;) legend(x = &quot;top&quot;, legend = c(&quot;MA, 3 points&quot;, &quot;White noise&quot;), lty = c(&quot;solid&quot;, &quot;dotted&quot;), col=c(&quot;brown&quot;, &quot;red&quot;), lwd = c(4,1), bty = &quot;n&quot;) The plot below shows a 7-point moving average. m7 &lt;- filter(x=w, filter=rep(x=1/7, times=7),method=&quot;convolution&quot;, sides=1) plot(x=w, ylab=expression(paste(m[t],&quot;or&quot;,w[t])), xlab=&quot;t&quot;,type=&quot;l&quot;,col=&quot;red&quot;,panel_filter=grid(col=&quot;gray&quot;,lty=&quot;dotted&quot;), lty=&quot;dotted&quot;) points(x=w, pch=20, col=&quot;blue&quot;) lines(x=m, col=&quot;brown&quot;, lty=&quot;solid&quot;,lwd=4) points(x=m, pch=20, col=&quot;orange&quot;) lines(x = m7, col = &quot;lightgreen&quot;, lty = &quot;solid&quot;, lwd = 4) points(x = m7, pch = 20, col = &quot;darkgreen&quot;) legend(x = &quot;top&quot;, legend = c(&quot;MA, 3 points&quot;, &quot;White noise&quot;, &quot;MA 7 points&quot;), lty = c(&quot;solid&quot;, &quot;dotted&quot;, &quot;solid&quot;), col = c(&quot;brown&quot;, &quot;red&quot;, &quot;lightgreen&quot;), lwd = c(2,1,4), bty=&quot;n&quot;) 4.3 Autoregression Example 4.3 Autoregression An autoregression model uses past observations to predict future observations in a regression model. Suppose the autoregression model is \\(x_t = 0.7x_{t-1} + w_t, w_t \\sim \\mathrm{i.i.d.} N(0,1) ,\\forall t = 1, …, n\\) Because there is one past period on the right hand side, this is often denoted as an AR(1) model Obviously, there will be a correlation between the random variables. set.seed(6381) #Different seed from white_noise.R w &lt;- rnorm(n = 200, mean = 0, sd = 1) head(w) #&gt; [1] 0.06737166 -0.68095839 0.78930605 0.60049855 #&gt; [5] -1.21297680 -1.14082872 #Simple way to simulate AR(1) data x &lt;- numeric(length = 200) x.1 &lt;- 0 for(i in 1:length(x)) { x[i] &lt;- 0.7*x.1 + w[i] x.1 &lt;- x[i] } head(data.frame(x, w)) #&gt; x w #&gt; 1 0.06737166 0.06737166 #&gt; 2 -0.63379823 -0.68095839 #&gt; 3 0.34564730 0.78930605 #&gt; 4 0.84245166 0.60049855 #&gt; 5 -0.62326064 -1.21297680 #&gt; 6 -1.57711117 -1.14082872 #Do not use first 100 x &lt;- x[101:200] dev.new(width = 8, height = 6, pointsize = 10) #Opens up wider plot window than the default (good for time series plots) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = c(&quot;red&quot;), lwd = 1 , main = expression(paste(&quot;AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t])), panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) #Show first few observations after removal of first 100 head(data.frame(c(NA, NA, x), w[99:200])) #&gt; c.NA..NA..x. w.99.200. #&gt; 1 NA 0.3918531 #&gt; 2 NA -0.1980565 #&gt; 3 1.429572 0.7508375 #&gt; 4 -1.878239 -2.8789389 #&gt; 5 -1.470250 -0.1554829 #&gt; 6 -3.464078 -2.4349033 #Correlation between x_t and x_t-1 cor(x[2:100], x[1:99]) #&gt; [1] 0.7270483 Here is an easier way to simulate observations from an AR(1). Note that this uses an Autoregressive Integrated Moving Average (ARIMA) structure that we will discuss later in the course. In this case, I use \\(\\sigma_w\\)= 10. set.seed(7181) x &lt;- arima.sim(model = list(ar = c(0.7)), n = 100, rand.gen = rnorm, sd = 10) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 ,main = expression(paste(&quot;AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t])), panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) "],["dependence.html", "Chapter 5 Dependence 5.1 Autocovariance function 5.2 Autocorrelation function (ACF) 5.3 Strong positive and negative linear dependence", " Chapter 5 Dependence We would like to understand the relationship among all random variables in a time series. In order to do that, we would need to look at the joint distribution function. Suppose the time series consists of the random variables . The cumulative joint distribution function for these random variables is: \\[ F(c_1, c_2, …, c_n) = P(X_1\\leq{c_1},X_2\\leq{c_2},...,X_n\\leq{c_n}) \\] This can be VERY difficult to examine over the MULTIDIMENSIONS. The one-dimensional cumulative distributional function is denoted by \\(F_t(x) = P(X_t\\leq{x})\\)for a random variable \\(X_t\\) at time t. The corresponding probability distribution function is \\(f_t(x)=\\frac{\\partial{F_t(x)}}{\\partial{x}}\\) The mean value function is \\(\\mu_t=E(X_t)=\\int{xf_t(x)dx}\\) Important: The interpretation of \\(\\mu_t\\) is that it represents the mean taken over ALL possible events that could have produced \\(x_t\\). Another way to think about it is suppose that \\(x_1,\\dots,x_n\\) is observed an infinite number of times. Then \\(\\mu_1\\) represents the average value at time 1, \\(\\mu_2\\) represents the average value at time 2, … Example 5.1 Moving Average Let \\(m_t=\\frac{(w_t+w_{t-1}+w_{t-2})}{3}\\), where \\(w_t\\sim\\mathrm{i.i.d.}N(0,1)\\forall t=1,...,n\\) Then \\(\\mu_t=E(m_t)=E[\\frac{(w_t+w_{t-1}+w_{t-2})}{3}]=\\frac{1}{3}[E(w_t)+E(w_{t-1})+E(w_{t-2})]=0\\) Example 5.2 Autoregressions Let \\(x_t = 0.7x_{t-1} + w_t, w_t \\sim \\mathrm{i.i.d.} N(0,1) ,\\forall t = 1, …, n\\) Then \\(\\mu_t=E(x_t)=E[0.7x_{t-1} + w_t]=0.7E(x_{t-1})+E(w_t)=0.7E(0.7x_{t-2}+w_{t-1})+0=...=0\\) 5.1 Autocovariance function To assess the dependence between two random variables, we need to examine the two-dimensional cumulative distribution function. This can be denoted as \\(F(c_s, c_t) = P(X_s\\leq{c_s}, X_t\\leq{c_t})\\)for two different time points s and t. In another course, you learned about the covariance function which measures the linear dependence between two random variables:For two random variables X and Y, the covariance between them is \\[Cov(X,Y) = E[(X – \\mu_x)(Y – \\mu_y)] = E(XY) – \\mu_x\\mu_y, \\mu_x = E(X) , \\mu_y = E(Y)\\] Because we are interested in linear dependence between two random variables in the same time series, we will examine the autocovariance function: \\[\\gamma (s,t) = Cov(X_s, X_t) = E[(X_s – \\mu_s)(X_t – \\mu_t)]= \\int\\int(x_s-\\mu_s)(x_t-\\mu_t)f(x_s,x_t)dx_sdx_t ,\\forall s, t \\] where\\(f(X_s,X_t)=\\frac{\\partial{F(X_s,X_t)}}{\\partial{X_s}\\partial{X_t}}\\)and assuming continuous random variables If the autocovariance is 0, there is no linear dependence. If s = t, the autocovariance is the variance: \\(\\gamma (t,t) = E[(X_t-\\mu_t)^2]\\) Example 5.3 White Noise Suppose \\(w_t \\sim ind. N(0,\\sigma^2_w ), t=1,…,n.\\) \\[\\begin{cases} \\gamma(s,t)=\\sigma^2_w &amp; \\text{if } s=t \\\\ \\gamma(s,t)= 0 &amp; \\text{if } s\\ne t \\end{cases}\\] Example 5.4 Moving Average Let \\(m_t=\\frac{w_t+w_{t-1}+w_{t-2}}{3}, w_t \\sim ind. N(0,1 ), t=1,…,n\\) \\(\\gamma (s,t)=E[(m_s-\\mu_s)(m_t-\\mu_t)]=E[m_sm_t]\\) b/c \\(\\mu_s=\\mu_t=0\\) Then \\(E[m_sm_t]=E[\\frac{w_s+w_{s-1}+w_{s-2}}{3}\\frac{w_t+w_{t-1}+w_{t-2}}{3}]=\\frac{1}{9}E[(w_s+w_{s-1}+w_{s-2})(w_t+w_{t-1}+w_{t-2})]\\) To find this, we need to examine a few different cases: s = t \\[E[m_tm_t] = E[m^2_t] = Var(m_t) + [E(m_t)]^2 = \\frac{1}{9}Var(w_t + w_{t-1} + w_{t-2}) + 0\\\\= \\frac{1}{9}{Var(w_t) + Var(w_{t-1}) + Var(w_{t-2})} = \\frac{1}{9}(1+1+1) = 3/9\\] s = t - 1 \\[E[m_{t-1}m_t] =\\frac{1}{9}E[(w_{t-1} + w_{t-2} + w_{t-3})(w_t + w_{t-1} + w_{t-2})]\\\\ = \\frac{1}{9}[E(w_{t-1})E(w_t) + E(w^2_{t-1}) + E(w_{t-1})E(w_{t-2}) \\\\ + E(w_{t-2})E(w_t) + E(w_{t-2})E(w_{t-1}) + E(w^2_{t-2}) \\\\ + E(w_{t-3})E(w_t) + E(w_{t-3})E(w_{t-1}) + E(w_{t-3})E(w_{t-2})]\\\\ =\\frac{1}{9}[0 + 1 + 0 + 0 + 0 + 1 + 0 + 0 + 0]=2/9\\] \\(E(w^2_{t-1}) = 1\\), because \\(Var(w_{t-1}) = 1\\) s = t - 2 \\[E[m_{t-2}m_t] = \\frac{1}{9}E[(w_{t-2} + w_{t-3} + w_{t-4})(w_t + w_{t-1} + w_{t-2})]\\\\ = \\frac{1}{9}[E(w_{t-2}w_t) + E(w_{t-2}w_{t-1}) + E(w_{t-2}w_{t-2}) +\\\\ E(w_{t-3}w_t) + E(w_{t-3}w_{t-1}) + E(w_{t-3}w_{t-2})\\\\ + E(w_{t-4}w_t) + E(w_{t-4}w_{t-1}) + E(w_{t-4}w_{t-2})]\\\\ =\\frac{1}{9}[0 + 0 + 0 + 1 + 0 + 0 + 0 + 0 + 0] =\\frac{1}{9}\\] s = t – 3 \\[ E[m_{t-3}m_t] = \\frac{1}{9}E[(w_{t-3} + w_{t-4} + w_{t-5})(w_t + w_{t-1} + w_{t-2})]\\\\ = \\frac{1}{9}E[w_{t-3}w_t + w_{t-3}w_{t-1} + w_{t-3}w_{t-2} + w_{t-4}w_t + w_{t-4}w_{t-1} \\\\ + w_{t-4}w_{t-2} + w_{t-5}w_t + w_{t-5}w_{t-1} + w_{t-5}w_{t-2}]\\\\ = \\frac{1}{9}E[0+ 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0]= 0 \\] Notice that s = t + 1, t + 2, … can be found in a similar manner. Also s = t - 4, t - 5,… can be found. In summary, the autocovariance function is\\[\\begin{cases} \\gamma(s,t)= \\frac{3}{9} &amp; \\text{if } s=t \\\\ \\gamma(s,t)= \\frac{2}{9} &amp; \\text{if } |s-t|=1\\\\ \\gamma(s,t)= \\frac{1}{9} &amp; \\text{if } |s-t|=2\\\\ \\gamma(s,t)= 0 &amp; \\text{if } |s-t|\\ge 3\\\\ \\end{cases}\\] Notes: - The word “lag” is used to denote the time separation between two values. For example, |s - t| = 1 denotes the lag is 1 and |s - t| = 2 denotes the lag is 2. We will use this “lag” terminology throughout this course. The autocovariance depends on the lag, but NOT individual times for the moving average example! This will be VERY, VERY important later! 5.2 Autocorrelation function (ACF) In another course, the Pearson correlation coefficient was defined to be: \\[\\rho=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] The reason the correlation coefficient is examined instead of the covariance is that it is always between –1 and 1. Note the following: close to 1 means strong, positive linear dependence close to –1 means strong, negative linear dependence close to 0 means weak linear dependence. The autocorrelation is the extension of the Pearson correlation coefficient to time series analysis. The autocorrelation function (ACF) is \\[\\rho(s,t)=\\frac{\\gamma (s,t)}{\\sqrt{\\gamma (s,s)\\gamma (t,t)}}\\] where s and t denote two time points. The ACF is also between –1 and 1 and has a similar interpretation as for correlation coefficient. Notice that \\(\\gamma (t,t)\\) is the variance at time t. Example 5.5 White Noise Suppose \\(w_t\\sim\\mathrm{ind} N(0,\\sigma^2_w), t=1,...,n\\) \\[\\begin{cases} \\rho(s,t)= \\frac{3}{9} &amp; \\text{if } s=t \\\\ \\rho(s,t)= \\frac{2}{9} &amp; \\text{if } s\\ne{t}\\\\ \\end{cases}\\] Example 5.6 Moving Average Let \\(m_t=\\frac{w_t+w_{t-1}+w_{t-2}}{3}, w_t\\sim\\mathrm{ind}N(0,1), t=1,...,n\\) \\[\\begin{cases} \\gamma(s,t)= \\frac{3}{9} &amp; \\text{if } s=t \\\\ \\gamma(s,t)= \\frac{2}{9} &amp; \\text{if } |s-t|=1\\\\ \\gamma(s,t)= \\frac{1}{9} &amp; \\text{if } |s-t|=2\\\\ \\gamma(s,t)= 0 &amp; \\text{if } |s-t|\\ge 3\\\\ \\end{cases}\\] \\[\\begin{cases} \\rho(s,t)= 1 &amp; \\text{if } s=t \\\\ \\rho(s,t)= \\frac{2}{3} &amp; \\text{if } |s-t|=1\\\\ \\rho(s,t)= \\frac{1}{3} &amp; \\text{if } |s-t|=2\\\\ \\rho(s,t)= 0 &amp; \\text{if } |s-t|\\ge 3\\\\ \\end{cases}\\] In par, if \\(|s-t|=1\\), then \\(\\rho(s,t)=\\frac{2/9}{\\sqrt{3/9}\\sqrt{3/9}}=\\frac{2}{3}\\) 5.3 Strong positive and negative linear dependence If there is strong positive linear dependence between \\(x_s\\) and \\(x_t\\), the time series will appear smooth in a plot of the series versus time. If there is strong negative linear dependence between \\(x_s\\) and \\(x_t\\), the time series will appear choppy in a plot of the series versus time. Below are three plots illustrating these statements. I simulated data from different time series models. The autocorrelation for |s - t| = 1 is given for each model. The “estimated” autocorrelation, denoted by \\(\\hat{\\rho}(s,t)\\) , is given for that particular data set. The calculation of this estimate will be discussed later. # Simulate a white noise series set.seed(4599) w &lt;- rnorm(n = 100, mean = 0, sd = 1) head(w) #&gt; [1] 0.2822308 0.0552224 0.9433917 0.6376982 1.4084635 #&gt; [6] 0.2050981 \\(\\rho(s,t)=0.4972, \\hat{\\rho}(s,t)=0.4705\\) for |s - t| = 1 dev.new(width = 8, height = 6, pointsize = 10) #Opens up wider plot window than the default (good for time series plots) #rho = 0.4972376 we will discuss why this is the numerical value later x &lt;- filter(x = w, filter = c(1,0.9), method = &quot;convolution&quot;, sides = 1) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 ,main = expression(paste(x[t] == w[t] + 0.9*w[t-1], &quot; where &quot;, w[t], &quot;~ ind. N(0,1)&quot;)) , panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) #Pearson correlation between x_t and x_t-1 cor(x = x[3:100], y = x[2:99]) #&gt; [1] 0.4704674 par(pty = &quot;s&quot;) # square plot #用於設置畫圖的視窗的約束比例（plotting window aspect ratio）。具體而言，pty 參數指定了圖形的寬度和高度之間的比例。 # &quot;s&quot; 表示圖形的寬度和高度應該按比例相等，也就是說，產生的圖形將是正方形，而不是預設的矩形。 plot(x = x[3:100], y = x[2:99], ylab = expression(x[t-1]), xlab = expression(x[t]), col = &quot;red&quot;, type = &quot;p&quot;, main = expression(paste(x[t-1], &quot;vs.&quot;, x[t])), panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) par(pty = &quot;m&quot;) # Return to normal \\(\\rho(s,t)=-0.4972, \\hat{\\rho}(s,t)=-0.5725\\) for |s - t| = 1 #rho = -0.4972376 x &lt;- filter(x = w, filter = c(1,-0.9), method = &quot;convolution&quot;, sides = 1) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 ,main = expression(paste(x[t] == w[t] - 0.9*w[t-1], &quot; where &quot;, w[t], &quot;~ ind. N(0,1)&quot;)) , panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) cor(x = x[3:100], y = x[2:99]) #&gt; [1] -0.5725393 \\(\\rho(s,t)=0, \\hat{\\rho}(s,t)=-0.1054\\) for |s - t| = 1 #rho = 0 x &lt;- filter(x = w, filter = c(1,0), method = &quot;convolution&quot;, sides = 1) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 ,main = expression(paste(x[t] == w[t] + 0*w[t-1], &quot; where &quot;, w[t], &quot;~ ind. N(0,1)&quot;)) , panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) cor(x = x[3:100], y = x[2:99]) #&gt; [1] -0.1053843 cor.test(x = x[3:100], y = x[2:99]) #Not significant #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: x[3:100] and x[2:99] #&gt; t = -1.0383, df = 96, p-value = 0.3017 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.29758249 0.09502345 #&gt; sample estimates: #&gt; cor #&gt; -0.1053843 Plot 1. is the least choppy (jagged) and plot 2. is the most choppy. Remember what a correlation means. A positive correlation means that “large” values tend to occur with other “large” values and “small” values tend to occur with other “small” values. A negative correlation means that “large” values tend to occur with other “small” values and “small” values tend to occur with other “large” values. "],["stationarity.html", "Chapter 6 Stationarity 6.1 Stationarity 6.2 Linear Process", " Chapter 6 Stationarity 6.1 Stationarity Stationarity is a VERY important concept to understand because it allow us to construct time series models. Definition 6.1 Strictly Stationary The probabilistic behavior of \\(X_{t_1},...,X_{t_k}\\) is exactly the same as that of the shifted set \\(X_{t_{1+h}},...,X_{t_{k+h}}\\) for ANY collection of time points t1,…, tk, for ANY k = 1, 2, …, and for ANY shift h = 0, \\(\\pm\\) 1, \\(\\pm\\) 2, … . Let \\(c_1,…, c_k\\) be constants. Then\\[P(X_{t_1}\\leq{c_1},...,X_{t_k}\\leq{c_k})=P(X_{t_{1+h}}\\leq{c_{1}},...,X_{t_{k+h}}\\leq{c_k})\\] i.e., the probability distribution is INVARIANT to time shifts. e.g. \\(P(X_1\\leq{c_1}, X_2\\leq{c_2})=P(X_{10}\\leq{c_1}, X_{11}\\leq{c_2})\\) Requiring a time series to be strictly stationary is VERY restrictive! A less restrictive requirement is weakly stationary. Definition 6.2 Weakly Stationary The first two moments (mean and covariance) of the time series are invariant to time shifts. \\[E(X_t)=\\mu, \\forall t\\] and \\[ \\gamma(t,t+h)=\\gamma(0,h), \\forall t\\] Notes: \\(\\mu\\) and \\(\\gamma(0,h)\\) are NOT functions of t. h is the lag \\(\\gamma(t, t+h)\\) = \\(\\gamma(0,h)\\) for ALL time t means that the autocovariance function ONLY depends on the number of lags of the time shift. Thus,\\(\\gamma(0,h) = \\gamma(1, h+1) = \\gamma(2, 2+h) = \\gamma(3, 3+h) = …\\) Because we will generally be dealing with a weakly stationary time series, we can make the following notational change: \\(\\gamma(h)=\\gamma(0,h)=\\gamma(t, t+h).\\) The variance of \\(X_t\\) is \\(\\gamma(0)\\). The same notational change can be made to the autocorrelation function (ACF). Thus, \\(\\rho(h)\\) denotes the ACF at lag h. Note that\\[\\rho(h)=\\rho(t,t+h)=\\frac{\\gamma(t,t+h)}{\\sqrt{\\gamma(t,t)}\\sqrt{\\gamma(t+h,t+h)}}=\\frac{\\gamma(h)}{\\sqrt{\\gamma(0)}\\sqrt{\\gamma(0)}}=\\frac{\\gamma(h)}{\\gamma(0)}\\] Strictly stationary implies weakly stationary, but the reverse is not necessarily true. Frequently, we will just say “stationary” to refer to weakly stationary and say the full “strictly stationary” to refer to strictly stationary. Example 6.1 White Noise Suppose \\(w_t \\sim ind. N(0,\\sigma^2_w ), t=1,…,n.\\) Is this a weakly stationary time series? Yes – mean is 0 for all wt, variance is constant for all wt, and the covariance is 0 because the random variables are independent. AND, it is also strictly stationary. Example 6.2 Moving Average Let \\(m_t=\\frac{w_t+w_{t-1}+w_{t-2}}{3}, w_t \\sim ind. N(0,1 ), t=1,…,n\\) Previously, we found that \\(\\mu_t=0, \\forall t\\) and \\[\\begin{cases} \\gamma(s,t)= \\frac{3}{9} &amp; \\text{if } s=t \\\\ \\gamma(s,t)= \\frac{2}{9} &amp; \\text{if } |s-t|=1\\\\ \\gamma(s,t)= \\frac{1}{9} &amp; \\text{if } |s-t|=2\\\\ \\gamma(s,t)= 0 &amp; \\text{if } |s-t|\\ge 3\\\\ \\end{cases}\\] Is the time series weakly stationary? Hint: let h=s-t. Yes! You can see that \\(\\gamma(s,t)\\) only depends on h. Comments: \\(\\gamma(h) = \\gamma(-h)\\) for all h if the series is weakly stationary. This means that it does not matter which way the shift occurs. Stationarity can also be examined when two time series are of interest. We will examine this in more detail later in the course. In summary, Both time series must have constant mean Both autocovariance functions must depend only on the lag difference The “cross-covariance” function, the extension of the autocovariance function for one time series to two time series, must depend only on the lag difference. The cross-covariance function is defined as \\[\\gamma_{xy}(h) = E[(x_{t+h} – \\mu_x)(y_t – \\mu_y)]\\] Note that \\(\\gamma_{xy}(h)\\) is not necessarily equal to \\(\\gamma_{yx}(h)\\) (usually will be different). Example 6.3 Let \\(x_t = w_t + w_{t-1}\\) and \\(y_t = w_t - w_{t-1}\\), where \\(w_t \\sim \\mathrm{ind}N(0,\\sigma^2_w )\\) for t = 1, …, n Claim: \\(x_t\\) and \\(y_t\\) are weakly stationary. Proof. \\(X_t\\) \\(E(X_t)=E(w_t+w_{t-1})=E(w_t+w_{t-1})=0+0=0\\) Thus \\(E(X_t)=\\mu_{X_t}=0 \\forall t\\) \\(\\gamma(s,t)=E[(X_s-\\mu_{X_s})(X_t-\\mu_{X_t})]\\\\=E[X_sX_t]=E[(w_s+w_{s+1})(w_t+w_{t+1})]\\\\=E[w_sw_t+w_{s-1}w_t+w_sw_{t-1}+w_{s-1}w_{t-1}]\\) if s=t, then \\(\\gamma(t,t)=E[w^2_t+ w_{t-1}w_t + w_tw_{t-1} + w^2_{t-1}]\\\\= Var(w_t) + (E[w_t])^2 + 2E[w_{t-1}]E[w_t] + Var(w_{t-1}) + (E[w_{t-1}])^2\\\\= \\sigma^2_w + 0 + 0 + \\sigma^2_w + 0= 2\\sigma^2_w\\) if s=t-1, then \\(\\gamma(t-1,t)=E[w_{t-1}w_t+ w_{t-2}w_t + w^2_{t-1} + w_{t-2}w_{t-1}]\\\\= E[w_{t-1}]E[w_t] + E[w_{t-2}]E[w_t] + Var(w_{t-1}) +(E[w_{t-1}])^2 + E[w_{t-2}]E[w_{t-1}]\\\\= 0 + 0 + \\sigma^2_w + 0+0=\\sigma^2_w\\) Note that \\(\\gamma(t - 1, t) = \\sigma^2_w\\) and \\(\\gamma(s,t) = 0\\) for |s - t|&gt;1. \\[\\begin{cases} \\gamma(s,t)= 2\\sigma^2_w &amp; \\text{if } s=t \\\\ \\gamma(s,t)= \\sigma^2_w &amp; \\text{if } |s-t|=1\\\\ \\gamma(s,t)= 0 &amp; \\text{if } |s-t| &gt; 1\\\\ \\end{cases}\\] \\(X_t\\) is weakly stationary. \\(Y_t\\) \\(E(Y_t)=E(w_t-w_{t-1})=E(w_t-w_{t-1})=0-0=0\\) Thus \\(E(Y_t)=\\mu_{Y_t}=0 \\forall t\\) \\(\\gamma(s,t)=E[(Y_s-\\mu_{Y_s})(Y_t-\\mu_{Y_t})]=E[Y_sY_t]\\\\=E[(w_s-w_{s+1})(w_t-w_{t+1})]=\\\\E[w_sw_t-w_{s-1}w_t-w_sw_{t-1}+w_{s-1}w_{t-1}]\\) if s=t, then \\(\\gamma(t,t)=E[w^2_t- w_{t-1}w_t - w_tw_{t-1} + w^2_{t-1}]\\\\= Var(w_t) + (E[w_t])^2 - 2E[w_{t-1}]E[w_t] + Var(w_{t-1}) + (E[w_{t-1}])^2\\\\= \\sigma^2_w + 0 - 0 + \\sigma^2_w + 0= 2\\sigma^2_w\\) if s=t-1, then \\(\\gamma(t-1,t)=E[w_{t-1}w_t- w_{t-2}w_t - w^2_{t-1} + w_{t-2}w_{t-1}]\\\\ = E[w_{t-1}]E[w_t] - E[w_{t-2}]E[w_t] - Var(w_{t-1}) -(E[w_{t-1}])^2 + E[w_{t-2}]E[w_{t-1}]\\\\= 0 - 0 - \\sigma^2_w - 0+0=-\\sigma^2_w\\) Note that \\(\\gamma(t - 1, t) = -\\sigma^2_w\\) and \\(\\gamma(s,t) = 0\\) for |s - t|&gt;1. \\[\\begin{cases} \\gamma(s,t)= 2\\sigma^2_w &amp; \\text{if } s=t \\\\ \\gamma(s,t)= -\\sigma^2_w &amp; \\text{if } |s-t|=1\\\\ \\gamma(s,t)= 0 &amp; \\text{if } |s-t| &gt; 1\\\\ \\end{cases}\\] \\(Y_t\\) is weakly stationary. 6.2 Linear Process The previous examples are special cases of a “linear process”. In general, a linear process can be defined as \\[X_t=\\mu+ \\sum_{j=-\\infty}^{\\infty} \\psi_jw_{t-j}, \\sum_{j=-\\infty}^{\\infty}|\\psi_j|&lt;\\infty\\] and \\[w_t\\sim\\mathrm{ind.}N(0,\\sigma^2_w)\\] It can be shown that \\(\\gamma(h)=\\sigma^2_w\\sum_{j=-\\infty}^{\\infty}\\psi_{j+h}\\psi_j\\) for h \\(\\geq\\) 0 provided the series is stationary (remember that and \\(\\gamma(h) = \\gamma(-h)\\)). Proof. WLOG, let \\(\\mu=0\\), since constants do not affect a covariance. \\(E(X_t)=E(\\mu+\\sum_{j=-\\infty}^{\\infty} \\psi_jw_{t-j})\\\\=0+\\sum_{j=-\\infty}^{\\infty} \\psi_jE(w_{t-j})\\\\=0+\\sum_{j=-\\infty}^{\\infty} \\psi_j0=0\\) Note that \\(\\gamma(h) = Cov(x_t, x_{t+h}) = E(x_tx_{t+h}) – E(x_t)E(x_{t+h}) = E(x_tx_{t+h})\\) since \\(E(x_t) = E(x_{t+h}) = 0\\) Then \\(E(x_tx_{t+h})=E[(\\sum_{i=-\\infty}^{\\infty}\\psi_iw_{t-i})(\\sum_{i=-\\infty}^{\\infty}\\psi_jw_{t+h-j})]\\\\=E[\\sum_{i=-\\infty}^{\\infty}\\sum_{j=-\\infty}^{\\infty}\\psi_i\\psi_jw_{t-i}w_{t+h-j}]\\\\=\\sum_{i=-\\infty}^{\\infty}\\sum_{j=-\\infty}^{\\infty}\\psi_i\\psi_jE(w_{t-i}w_{t+h-j})\\\\=\\sigma^2_w\\sum_{k=-\\infty}^{\\infty}\\psi_k\\psi_{k+h}\\) Note that \\(E(w_{t-i}w_{t+h-j})=0, \\forall -i\\ne{h-j}\\) and \\(E(w_{t-i}w_{t+h-j})=E(w^2_{t-i})=\\sigma^2_w ,\\forall -i=h-j ( i.e.,j-i=h)\\) and \\(\\psi&#39;s\\) are constants. Hence, \\(\\gamma(h)=\\sigma^2_w\\sum_{j=-\\infty}^{\\infty}\\psi_{j+h}\\psi_j\\) for h \\(\\geq\\) 0. Important: There is a very important case when weakly stationary implies strictly stationary. This occurs when the time series has a multivariate normal distribution. Remember that a univariate normal distribution is defined only by its mean and variance. The multivariate normal distribution is defined only by its mean vector and covariance matrix. Thus, if we can assume a multivariate normal distribution, we ONLY need to check if the time series satisfies the weakly stationary requirements to say the time series is strongly stationary. Thus, notice what the word “stationary” would mean in this case. Example 6.4 Visualizing Stationarity Below are a few plots of the observed values of a time series. Identify which plots correspond to a weakly stationary series. This graph violates weakly stationary. It seems that there’s a structural change. You can find there are two distinct mean: high mean and low mean. This graph doesn’t seem to violate weakly stationary, as you can see the mean seems to be a constant 0 and the variablity seems to be a constant across different time points. Clearly, this violates weakly stationary. You can see the mean keeps changing. Clearly, this violates weakly stationary. You can see the variance keeps changing. It is important to have stationarity b/c it allows us to estimate the mean and variance and we need consistency to construct a model. "],["estimation-and-inference-for-measures-of-dependence.html", "Chapter 7 Estimation and Inference for measures of dependence 7.1 Sample mean function 7.2 Sample autocovariance function 7.3 Sample autocorrelation function (ACF) 7.4 Sampling distribution", " Chapter 7 Estimation and Inference for measures of dependence \\(\\mu, \\gamma(h),\\rho(h)\\) are usually unknown so we need to estimate them. To estimate these quantities, we need to assume the time series is weakly stationary. 7.1 Sample mean function By the weakly stationary assumption, \\(E(x_1) = \\mu, E(x_2) = \\mu,…, E(x_n) = \\mu\\). Thus, a logical estimate of \\(\\mu\\) is\\[\\bar{X}=\\frac{1}{n}\\sum_{t=1}^{n}X_t\\] Note that this would not make sense to do if the weakly stationarity assumption did not hold! 7.2 Sample autocovariance function Again with the weakly stationarity assumption, we only need to worry about the lag difference. The estimated autocovariance function is: \\[\\hat{\\gamma}(h)=\\frac{1}{n}\\sum_{t=1}^{n-h}(X_{t+h}-\\bar{X})(X_t-\\bar{X})\\] \\(\\hat{\\gamma}(h)=\\hat{\\gamma}(-h)\\) What is this quantity if h = 0? \\(\\hat{\\gamma}(h)=\\frac{1}{n}\\sum_{t=1}^{n}(X_t-\\bar{X})(X_t-\\bar{X})\\), which is essentially the sample variance.(when n is large n\\(\\approx\\)n-1) What is this quantity if h =1? \\(\\hat{\\gamma}(h)=\\frac{1}{n}\\sum_{t=1}^{n-1}(X_{t+1}-\\bar{X})(X_t-\\bar{X})\\) This is similar to the formula often used to estimate the covariance between two random variables x and y: \\(\\hat{Cov}(X,Y)=\\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\bar{X})(Y_i-\\bar{Y})\\). The sum goes up to n - h to avoid having negative subscripts in the x’s. This is NOT an unbiased estimate of \\(\\gamma(h)\\)! However, as n gets larger, the bias will go to 0. 7.3 Sample autocorrelation function (ACF) \\[\\hat{\\rho}(h)=\\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)}\\] Question: What does \\(\\rho(h) = 0\\) mean and why would this be important to detect? That means there’s no linear relationship between \\(X_{t-h}\\) and \\(X_t\\) for this particular lag h. This is important b/c this makes it no sense to use \\(X_{t-h}\\) to predict \\(X_t\\). Because this is important, we conduct hypothesis tests for \\(\\rho(h)\\) for all h \\(\\ne\\) 0! To do the hypothesis test, we need to find the sampling distribution for \\(\\hat{\\rho}(h)\\) under the null hypothesis of \\(\\rho(h)\\) = 0. 7.4 Sampling distribution In summary, if \\(\\rho(h)\\) = 0, \\(x_t\\) is stationary, and the sample size is “large”, then \\(\\hat{\\rho}(h)\\) has an approximate normal distribution with mean 0 and standard deviation \\(\\sigma_{\\hat{\\rho}(h)}=\\frac{1}{\\sqrt{n}}\\), i.e., \\(\\hat{\\rho}(h)\\sim N(0,\\frac{1}{\\sqrt{n}})\\) A proof is available in Shumway and Stoffer’s textbook and requires an understanding asymptotics (PhD level statistics course). (\\(\\sqrt{n}(\\hat{\\rho}(h)-0) \\to^{d} N(0,1)\\)) \\(H_0: \\rho(h)=0\\) \\(Z=\\frac{\\hat{\\rho}(h)-0}{\\frac{1}{\\sqrt{n}}}\\) \\(Z&gt;|Z_{1-\\frac{\\alpha}{2}}|\\) reject \\(H_0\\) \\(\\hat{\\rho}(h)&gt;\\pm \\frac{Z_{1-\\frac{\\alpha}{2}}}{\\sqrt{n}}\\) reject \\(H_0\\) For a hypothesis test, we could check if \\(\\hat{\\rho}(h)\\) is within the bounds of 0 \\(\\pm \\frac{Z_{1-\\frac{\\alpha}{2}}}{\\sqrt{n}}\\) or not where P(Z &lt; \\(Z_{1-\\frac{\\alpha}{2}}\\)) = 1 – \\(\\frac{\\alpha}{2}\\) for a standard normal random variable Z. If it is not, then there is sufficient evidence to conclude that \\(\\rho(h) \\ne\\) 0. We will be using this result a lot for the rest of this course! Example 7.1 \\(x_t=0.7x_{t-1}+w_t, w_t\\sim\\mathrm{ind.}N(0,1), n=100\\) Click here to download data. ar1 &lt;- read.table(file = &quot;AR1.0.7.txt&quot;, header = TRUE, sep = &quot;&quot;) head(ar1) #&gt; t x #&gt; 1 1 0.0417268 #&gt; 2 2 0.3719068 #&gt; 3 3 -0.1854518 #&gt; 4 4 -1.3829742 #&gt; 5 5 -2.8759365 #&gt; 6 6 -2.6001761 x &lt;- ar1$x dev.new(width = 8, height = 6, pointsize = 10) #Opens up wider plot window than the default (good for time series plots) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 , main = expression(paste(&quot;Data simulated from AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;)) , panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) The easiest way to find the autocorrelations in R is to use the acf() function. rho.x &lt;- acf(x = x, type = &quot;correlation&quot;, main = expression(paste(&quot;Data simulated from AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;))) # ci argument can be used to change 1 - alpha for plot # lag.max argument can be used to change the maximum number of lags In our language, the horizontal axis: lag=h, the verical axis: ACF=\\(\\hat{\\rho}(h)\\) The horizontal lines on the plot are drawn at 0 \\(\\pm \\frac{Z_{1-\\frac{0.05}{2}}}{\\sqrt{n}}\\) where \\(Z_{1-\\frac{0.05}{2}} = 1.96\\). i.e., outside the blue dashed line, we reject \\(H_0\\) The location of the lines can be changed by using the ci (confidence interval )argument. The default is ci = 0.95. i.e., \\(\\alpha=0.05\\) rho.x #&gt; #&gt; Autocorrelations of series &#39;x&#39;, by lag #&gt; #&gt; 0 1 2 3 4 5 6 7 #&gt; 1.000 0.674 0.401 0.169 -0.023 -0.125 -0.067 -0.064 #&gt; 8 9 10 11 12 13 14 15 #&gt; -0.058 0.005 -0.044 -0.041 -0.017 0.064 0.076 0.160 #&gt; 16 17 18 19 20 #&gt; 0.191 0.141 0.081 0.006 -0.132 # the first one is rho_hat(0) # the second one is rho_hat(1) names(rho.x) #&gt; [1] &quot;acf&quot; &quot;type&quot; &quot;n.used&quot; &quot;lag&quot; &quot;series&quot; &quot;snames&quot; rho.x$acf #&gt; , , 1 #&gt; #&gt; [,1] #&gt; [1,] 1.000000000 #&gt; [2,] 0.673671871 #&gt; [3,] 0.400891188 #&gt; [4,] 0.168552826 #&gt; [5,] -0.023391129 #&gt; [6,] -0.124632501 #&gt; [7,] -0.067392830 #&gt; [8,] -0.064248086 #&gt; [9,] -0.057717749 #&gt; [10,] 0.005312358 #&gt; [11,] -0.044035976 #&gt; [12,] -0.041121407 #&gt; [13,] -0.017197132 #&gt; [14,] 0.063864970 #&gt; [15,] 0.075575696 #&gt; [16,] 0.159665692 #&gt; [17,] 0.191349965 #&gt; [18,] 0.140967540 #&gt; [19,] 0.080508273 #&gt; [20,] 0.005584061 #&gt; [21,] -0.131559629 # the first one is rho_hat(0) # the second one is rho_hat(1) rho.x$acf[1:2] #&gt; [1] 1.0000000 0.6736719 Questions: What happens to the autocorrelations over time? Why do you think this happens? From the model \\(x_t=0.7x_{t-1}+w_t\\), you can see that the auto correlation dies out as the lag term h increases, the main reason is the coefficient 0.7 Is there a positive or negative correlation? A positive correlation, again from our model \\(x_t=0.7x_{t-1}+w_t\\), 0.7&gt;0 At what lags is $(h)$0? h=0,1,2. But we don’t care h=0, it’s 1 just by definition. R plots \\(\\hat{\\rho}(0)=1\\) by default. This is unnecessary because \\(\\hat{\\rho}(0)\\) will be 1 for all time series data sets (again, it’s just by definition)! To remove \\(\\hat{\\rho}(0)\\) from the plot, one can specify the x-axis limit to start at 1. Below is one way this can be done and also illustrates how to use the lag.max argument. par(xaxs = &quot;i&quot;) # Remove default 4% extra space around min and max of x-axis rho.x2 &lt;- acf(x = x, type = &quot;correlation&quot;, xlim = c(0,30), lag.max = 30, main = expression(paste(&quot;Data simulated from AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;))) par(xaxs = &quot;r&quot;) # Return to the default: regular pattern Note that \\(\\hat{\\rho}(0)=1\\) is still present but the y-axis at x = 0 hides it. While displaying \\(\\hat{\\rho}(0)=1\\) may seem minor, we will examine these autocorrelations later in the course to determine an appropriate model for a data set. Often, one will forget to ignore the line drawn at lag = 0 and choose an incorrect model. You should always ignore the line drawn at lag=0!!! b/c it’s 1 just by definition. The autocovariances can also be found using acf(). acf(x = x, type = &quot;covariance&quot;, main = expression(paste(&quot;Data simulated from AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;))) To help understand autocorrelations and their relationship with the correlation coefficient better, I decided to look at the “usual” estimated Pearson correlation coefficients between \\(x_t, x_{t-1}, x_{t-2},\\) and \\(x_{t-3}\\). # Examine usual ways to check correlation x.ts &lt;- ts(x) x.ts #&gt; Time Series: #&gt; Start = 1 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] 0.04172680 0.37190682 -0.18545185 -1.38297422 #&gt; [5] -2.87593652 -2.60017605 -1.10401719 -0.46385116 #&gt; [9] 0.80339069 2.11483585 3.39124978 3.86739194 #&gt; [13] 2.12733595 0.67590604 0.71429367 0.38928044 #&gt; [17] -1.22681923 0.02287443 -0.57321924 -2.68376851 #&gt; [21] -4.80850095 -2.44797633 -1.73921817 -1.48773023 #&gt; [25] 0.68920966 0.40220308 -1.58781041 -2.07300848 #&gt; [29] -2.63408197 -3.54333559 -2.74116328 -2.54401750 #&gt; [33] -3.19570817 -0.02623669 0.41663974 -1.74485791 #&gt; [37] -3.04847994 -1.64692948 -1.71478199 0.11340965 #&gt; [41] 1.55017869 0.47317192 -0.18521791 -0.05759091 #&gt; [45] -1.32105323 -1.22071881 -1.53827085 0.01277076 #&gt; [49] -3.19955388 -3.11022833 -3.30621969 -2.00546537 #&gt; [53] 0.18084415 0.58776479 -0.70238414 -0.34570939 #&gt; [57] 0.94209248 -0.78176791 1.30547320 -1.04054783 #&gt; [61] -1.06897160 -1.08850000 0.06031172 -0.05724856 #&gt; [65] -1.14731083 -0.79262221 -0.55565451 -1.55985750 #&gt; [69] -2.17062644 -1.07776017 0.51569067 2.30660050 #&gt; [73] 1.53530426 2.55899301 1.83836277 1.08072014 #&gt; [77] 1.34125182 -0.80729300 -1.42735924 -0.42456207 #&gt; [81] -0.11625003 -0.74807460 0.70052717 0.08557377 #&gt; [85] -0.06039041 0.04479407 -0.12657328 -1.30097021 #&gt; [89] 0.81586192 -0.13139757 1.84725644 1.62364752 #&gt; [93] 0.33080663 -0.40824385 -1.56008530 -1.63175408 #&gt; [97] -1.36418639 -0.37209392 -0.65833401 2.03705932 set1 &lt;- ts.intersect(x.ts, x.ts1 = lag(x = x.ts, k = -1), x.ts2 = lag(x = x.ts, k = -2), x.ts3 = lag(x = x.ts, k = -3)) # b/c we use ts.intersect (take intersection), we have the following # x.ts starts at X4 # x.ts1 starts at X3 # x.ts2 starts at X2 # x.ts3 starts at X1 set1 #&gt; Time Series: #&gt; Start = 4 #&gt; End = 100 #&gt; Frequency = 1 #&gt; x.ts x.ts1 x.ts2 x.ts3 #&gt; 4 -1.38297422 -0.18545185 0.37190682 0.04172680 #&gt; 5 -2.87593652 -1.38297422 -0.18545185 0.37190682 #&gt; 6 -2.60017605 -2.87593652 -1.38297422 -0.18545185 #&gt; 7 -1.10401719 -2.60017605 -2.87593652 -1.38297422 #&gt; 8 -0.46385116 -1.10401719 -2.60017605 -2.87593652 #&gt; 9 0.80339069 -0.46385116 -1.10401719 -2.60017605 #&gt; 10 2.11483585 0.80339069 -0.46385116 -1.10401719 #&gt; 11 3.39124978 2.11483585 0.80339069 -0.46385116 #&gt; 12 3.86739194 3.39124978 2.11483585 0.80339069 #&gt; 13 2.12733595 3.86739194 3.39124978 2.11483585 #&gt; 14 0.67590604 2.12733595 3.86739194 3.39124978 #&gt; 15 0.71429367 0.67590604 2.12733595 3.86739194 #&gt; 16 0.38928044 0.71429367 0.67590604 2.12733595 #&gt; 17 -1.22681923 0.38928044 0.71429367 0.67590604 #&gt; 18 0.02287443 -1.22681923 0.38928044 0.71429367 #&gt; 19 -0.57321924 0.02287443 -1.22681923 0.38928044 #&gt; 20 -2.68376851 -0.57321924 0.02287443 -1.22681923 #&gt; 21 -4.80850095 -2.68376851 -0.57321924 0.02287443 #&gt; 22 -2.44797633 -4.80850095 -2.68376851 -0.57321924 #&gt; 23 -1.73921817 -2.44797633 -4.80850095 -2.68376851 #&gt; 24 -1.48773023 -1.73921817 -2.44797633 -4.80850095 #&gt; 25 0.68920966 -1.48773023 -1.73921817 -2.44797633 #&gt; 26 0.40220308 0.68920966 -1.48773023 -1.73921817 #&gt; 27 -1.58781041 0.40220308 0.68920966 -1.48773023 #&gt; 28 -2.07300848 -1.58781041 0.40220308 0.68920966 #&gt; 29 -2.63408197 -2.07300848 -1.58781041 0.40220308 #&gt; 30 -3.54333559 -2.63408197 -2.07300848 -1.58781041 #&gt; 31 -2.74116328 -3.54333559 -2.63408197 -2.07300848 #&gt; 32 -2.54401750 -2.74116328 -3.54333559 -2.63408197 #&gt; 33 -3.19570817 -2.54401750 -2.74116328 -3.54333559 #&gt; 34 -0.02623669 -3.19570817 -2.54401750 -2.74116328 #&gt; 35 0.41663974 -0.02623669 -3.19570817 -2.54401750 #&gt; 36 -1.74485791 0.41663974 -0.02623669 -3.19570817 #&gt; 37 -3.04847994 -1.74485791 0.41663974 -0.02623669 #&gt; 38 -1.64692948 -3.04847994 -1.74485791 0.41663974 #&gt; 39 -1.71478199 -1.64692948 -3.04847994 -1.74485791 #&gt; 40 0.11340965 -1.71478199 -1.64692948 -3.04847994 #&gt; 41 1.55017869 0.11340965 -1.71478199 -1.64692948 #&gt; 42 0.47317192 1.55017869 0.11340965 -1.71478199 #&gt; 43 -0.18521791 0.47317192 1.55017869 0.11340965 #&gt; 44 -0.05759091 -0.18521791 0.47317192 1.55017869 #&gt; 45 -1.32105323 -0.05759091 -0.18521791 0.47317192 #&gt; 46 -1.22071881 -1.32105323 -0.05759091 -0.18521791 #&gt; 47 -1.53827085 -1.22071881 -1.32105323 -0.05759091 #&gt; 48 0.01277076 -1.53827085 -1.22071881 -1.32105323 #&gt; 49 -3.19955388 0.01277076 -1.53827085 -1.22071881 #&gt; 50 -3.11022833 -3.19955388 0.01277076 -1.53827085 #&gt; 51 -3.30621969 -3.11022833 -3.19955388 0.01277076 #&gt; 52 -2.00546537 -3.30621969 -3.11022833 -3.19955388 #&gt; 53 0.18084415 -2.00546537 -3.30621969 -3.11022833 #&gt; 54 0.58776479 0.18084415 -2.00546537 -3.30621969 #&gt; 55 -0.70238414 0.58776479 0.18084415 -2.00546537 #&gt; 56 -0.34570939 -0.70238414 0.58776479 0.18084415 #&gt; 57 0.94209248 -0.34570939 -0.70238414 0.58776479 #&gt; 58 -0.78176791 0.94209248 -0.34570939 -0.70238414 #&gt; 59 1.30547320 -0.78176791 0.94209248 -0.34570939 #&gt; 60 -1.04054783 1.30547320 -0.78176791 0.94209248 #&gt; 61 -1.06897160 -1.04054783 1.30547320 -0.78176791 #&gt; 62 -1.08850000 -1.06897160 -1.04054783 1.30547320 #&gt; 63 0.06031172 -1.08850000 -1.06897160 -1.04054783 #&gt; 64 -0.05724856 0.06031172 -1.08850000 -1.06897160 #&gt; 65 -1.14731083 -0.05724856 0.06031172 -1.08850000 #&gt; 66 -0.79262221 -1.14731083 -0.05724856 0.06031172 #&gt; 67 -0.55565451 -0.79262221 -1.14731083 -0.05724856 #&gt; 68 -1.55985750 -0.55565451 -0.79262221 -1.14731083 #&gt; 69 -2.17062644 -1.55985750 -0.55565451 -0.79262221 #&gt; 70 -1.07776017 -2.17062644 -1.55985750 -0.55565451 #&gt; 71 0.51569067 -1.07776017 -2.17062644 -1.55985750 #&gt; 72 2.30660050 0.51569067 -1.07776017 -2.17062644 #&gt; 73 1.53530426 2.30660050 0.51569067 -1.07776017 #&gt; 74 2.55899301 1.53530426 2.30660050 0.51569067 #&gt; 75 1.83836277 2.55899301 1.53530426 2.30660050 #&gt; 76 1.08072014 1.83836277 2.55899301 1.53530426 #&gt; 77 1.34125182 1.08072014 1.83836277 2.55899301 #&gt; 78 -0.80729300 1.34125182 1.08072014 1.83836277 #&gt; 79 -1.42735924 -0.80729300 1.34125182 1.08072014 #&gt; 80 -0.42456207 -1.42735924 -0.80729300 1.34125182 #&gt; 81 -0.11625003 -0.42456207 -1.42735924 -0.80729300 #&gt; 82 -0.74807460 -0.11625003 -0.42456207 -1.42735924 #&gt; 83 0.70052717 -0.74807460 -0.11625003 -0.42456207 #&gt; 84 0.08557377 0.70052717 -0.74807460 -0.11625003 #&gt; 85 -0.06039041 0.08557377 0.70052717 -0.74807460 #&gt; 86 0.04479407 -0.06039041 0.08557377 0.70052717 #&gt; 87 -0.12657328 0.04479407 -0.06039041 0.08557377 #&gt; 88 -1.30097021 -0.12657328 0.04479407 -0.06039041 #&gt; 89 0.81586192 -1.30097021 -0.12657328 0.04479407 #&gt; 90 -0.13139757 0.81586192 -1.30097021 -0.12657328 #&gt; 91 1.84725644 -0.13139757 0.81586192 -1.30097021 #&gt; 92 1.62364752 1.84725644 -0.13139757 0.81586192 #&gt; 93 0.33080663 1.62364752 1.84725644 -0.13139757 #&gt; 94 -0.40824385 0.33080663 1.62364752 1.84725644 #&gt; 95 -1.56008530 -0.40824385 0.33080663 1.62364752 #&gt; 96 -1.63175408 -1.56008530 -0.40824385 0.33080663 #&gt; 97 -1.36418639 -1.63175408 -1.56008530 -0.40824385 #&gt; 98 -0.37209392 -1.36418639 -1.63175408 -1.56008530 #&gt; 99 -0.65833401 -0.37209392 -1.36418639 -1.63175408 #&gt; 100 2.03705932 -0.65833401 -0.37209392 -1.36418639 cor(set1) #&gt; x.ts x.ts1 x.ts2 x.ts3 #&gt; x.ts 1.0000000 0.6824913 0.4065326 0.1710145 #&gt; x.ts1 0.6824913 1.0000000 0.6929638 0.4108375 #&gt; x.ts2 0.4065326 0.6929638 1.0000000 0.6935801 #&gt; x.ts3 0.1710145 0.4108375 0.6935801 1.0000000 # corr matrix library(car) #scatterplot.matrix is in this package #&gt; Loading required package: carData scatterplotMatrix(formula = ~x.ts + x.ts1 + x.ts2 + x.ts3, data = set1, diagonal = list(method = &quot;histogram&quot;), col = &quot;red&quot;) set2 &lt;- ts.intersect(x.ts, x.ts1 = lag(x = x.ts, k = 1), x.ts2 = lag(x = x.ts, k = 2), x.ts3 = lag(x = x.ts, k = 3)) set2 #&gt; Time Series: #&gt; Start = 1 #&gt; End = 97 #&gt; Frequency = 1 #&gt; x.ts x.ts1 x.ts2 x.ts3 #&gt; 1 0.04172680 0.37190682 -0.18545185 -1.38297422 #&gt; 2 0.37190682 -0.18545185 -1.38297422 -2.87593652 #&gt; 3 -0.18545185 -1.38297422 -2.87593652 -2.60017605 #&gt; 4 -1.38297422 -2.87593652 -2.60017605 -1.10401719 #&gt; 5 -2.87593652 -2.60017605 -1.10401719 -0.46385116 #&gt; 6 -2.60017605 -1.10401719 -0.46385116 0.80339069 #&gt; 7 -1.10401719 -0.46385116 0.80339069 2.11483585 #&gt; 8 -0.46385116 0.80339069 2.11483585 3.39124978 #&gt; 9 0.80339069 2.11483585 3.39124978 3.86739194 #&gt; 10 2.11483585 3.39124978 3.86739194 2.12733595 #&gt; 11 3.39124978 3.86739194 2.12733595 0.67590604 #&gt; 12 3.86739194 2.12733595 0.67590604 0.71429367 #&gt; 13 2.12733595 0.67590604 0.71429367 0.38928044 #&gt; 14 0.67590604 0.71429367 0.38928044 -1.22681923 #&gt; 15 0.71429367 0.38928044 -1.22681923 0.02287443 #&gt; 16 0.38928044 -1.22681923 0.02287443 -0.57321924 #&gt; 17 -1.22681923 0.02287443 -0.57321924 -2.68376851 #&gt; 18 0.02287443 -0.57321924 -2.68376851 -4.80850095 #&gt; 19 -0.57321924 -2.68376851 -4.80850095 -2.44797633 #&gt; 20 -2.68376851 -4.80850095 -2.44797633 -1.73921817 #&gt; 21 -4.80850095 -2.44797633 -1.73921817 -1.48773023 #&gt; 22 -2.44797633 -1.73921817 -1.48773023 0.68920966 #&gt; 23 -1.73921817 -1.48773023 0.68920966 0.40220308 #&gt; 24 -1.48773023 0.68920966 0.40220308 -1.58781041 #&gt; 25 0.68920966 0.40220308 -1.58781041 -2.07300848 #&gt; 26 0.40220308 -1.58781041 -2.07300848 -2.63408197 #&gt; 27 -1.58781041 -2.07300848 -2.63408197 -3.54333559 #&gt; 28 -2.07300848 -2.63408197 -3.54333559 -2.74116328 #&gt; 29 -2.63408197 -3.54333559 -2.74116328 -2.54401750 #&gt; 30 -3.54333559 -2.74116328 -2.54401750 -3.19570817 #&gt; 31 -2.74116328 -2.54401750 -3.19570817 -0.02623669 #&gt; 32 -2.54401750 -3.19570817 -0.02623669 0.41663974 #&gt; 33 -3.19570817 -0.02623669 0.41663974 -1.74485791 #&gt; 34 -0.02623669 0.41663974 -1.74485791 -3.04847994 #&gt; 35 0.41663974 -1.74485791 -3.04847994 -1.64692948 #&gt; 36 -1.74485791 -3.04847994 -1.64692948 -1.71478199 #&gt; 37 -3.04847994 -1.64692948 -1.71478199 0.11340965 #&gt; 38 -1.64692948 -1.71478199 0.11340965 1.55017869 #&gt; 39 -1.71478199 0.11340965 1.55017869 0.47317192 #&gt; 40 0.11340965 1.55017869 0.47317192 -0.18521791 #&gt; 41 1.55017869 0.47317192 -0.18521791 -0.05759091 #&gt; 42 0.47317192 -0.18521791 -0.05759091 -1.32105323 #&gt; 43 -0.18521791 -0.05759091 -1.32105323 -1.22071881 #&gt; 44 -0.05759091 -1.32105323 -1.22071881 -1.53827085 #&gt; 45 -1.32105323 -1.22071881 -1.53827085 0.01277076 #&gt; 46 -1.22071881 -1.53827085 0.01277076 -3.19955388 #&gt; 47 -1.53827085 0.01277076 -3.19955388 -3.11022833 #&gt; 48 0.01277076 -3.19955388 -3.11022833 -3.30621969 #&gt; 49 -3.19955388 -3.11022833 -3.30621969 -2.00546537 #&gt; 50 -3.11022833 -3.30621969 -2.00546537 0.18084415 #&gt; 51 -3.30621969 -2.00546537 0.18084415 0.58776479 #&gt; 52 -2.00546537 0.18084415 0.58776479 -0.70238414 #&gt; 53 0.18084415 0.58776479 -0.70238414 -0.34570939 #&gt; 54 0.58776479 -0.70238414 -0.34570939 0.94209248 #&gt; 55 -0.70238414 -0.34570939 0.94209248 -0.78176791 #&gt; 56 -0.34570939 0.94209248 -0.78176791 1.30547320 #&gt; 57 0.94209248 -0.78176791 1.30547320 -1.04054783 #&gt; 58 -0.78176791 1.30547320 -1.04054783 -1.06897160 #&gt; 59 1.30547320 -1.04054783 -1.06897160 -1.08850000 #&gt; 60 -1.04054783 -1.06897160 -1.08850000 0.06031172 #&gt; 61 -1.06897160 -1.08850000 0.06031172 -0.05724856 #&gt; 62 -1.08850000 0.06031172 -0.05724856 -1.14731083 #&gt; 63 0.06031172 -0.05724856 -1.14731083 -0.79262221 #&gt; 64 -0.05724856 -1.14731083 -0.79262221 -0.55565451 #&gt; 65 -1.14731083 -0.79262221 -0.55565451 -1.55985750 #&gt; 66 -0.79262221 -0.55565451 -1.55985750 -2.17062644 #&gt; 67 -0.55565451 -1.55985750 -2.17062644 -1.07776017 #&gt; 68 -1.55985750 -2.17062644 -1.07776017 0.51569067 #&gt; 69 -2.17062644 -1.07776017 0.51569067 2.30660050 #&gt; 70 -1.07776017 0.51569067 2.30660050 1.53530426 #&gt; 71 0.51569067 2.30660050 1.53530426 2.55899301 #&gt; 72 2.30660050 1.53530426 2.55899301 1.83836277 #&gt; 73 1.53530426 2.55899301 1.83836277 1.08072014 #&gt; 74 2.55899301 1.83836277 1.08072014 1.34125182 #&gt; 75 1.83836277 1.08072014 1.34125182 -0.80729300 #&gt; 76 1.08072014 1.34125182 -0.80729300 -1.42735924 #&gt; 77 1.34125182 -0.80729300 -1.42735924 -0.42456207 #&gt; 78 -0.80729300 -1.42735924 -0.42456207 -0.11625003 #&gt; 79 -1.42735924 -0.42456207 -0.11625003 -0.74807460 #&gt; 80 -0.42456207 -0.11625003 -0.74807460 0.70052717 #&gt; 81 -0.11625003 -0.74807460 0.70052717 0.08557377 #&gt; 82 -0.74807460 0.70052717 0.08557377 -0.06039041 #&gt; 83 0.70052717 0.08557377 -0.06039041 0.04479407 #&gt; 84 0.08557377 -0.06039041 0.04479407 -0.12657328 #&gt; 85 -0.06039041 0.04479407 -0.12657328 -1.30097021 #&gt; 86 0.04479407 -0.12657328 -1.30097021 0.81586192 #&gt; 87 -0.12657328 -1.30097021 0.81586192 -0.13139757 #&gt; 88 -1.30097021 0.81586192 -0.13139757 1.84725644 #&gt; 89 0.81586192 -0.13139757 1.84725644 1.62364752 #&gt; 90 -0.13139757 1.84725644 1.62364752 0.33080663 #&gt; 91 1.84725644 1.62364752 0.33080663 -0.40824385 #&gt; 92 1.62364752 0.33080663 -0.40824385 -1.56008530 #&gt; 93 0.33080663 -0.40824385 -1.56008530 -1.63175408 #&gt; 94 -0.40824385 -1.56008530 -1.63175408 -1.36418639 #&gt; 95 -1.56008530 -1.63175408 -1.36418639 -0.37209392 #&gt; 96 -1.63175408 -1.36418639 -0.37209392 -0.65833401 #&gt; 97 -1.36418639 -0.37209392 -0.65833401 2.03705932 cor(set2) #&gt; x.ts x.ts1 x.ts2 x.ts3 #&gt; x.ts 1.0000000 0.6935801 0.4108375 0.1710145 #&gt; x.ts1 0.6935801 1.0000000 0.6929638 0.4065326 #&gt; x.ts2 0.4108375 0.6929638 1.0000000 0.6824913 #&gt; x.ts3 0.1710145 0.4065326 0.6824913 1.0000000 #Another way to see dependence lag.plot(x = x, lags = 4, layout = c(2,2), main = &quot;x vs. lagged x&quot;, do.lines = FALSE) The ts() function converts the time series data to an object that R recognizes as a time series. The lag() function is used to find xt-1, xt-2, and xt-3. The k argument specifies how many time periods to go back. Run lag(x.ts, k = -1) and lag(x.ts, k = 1)to see what happens. To get everything lined up as I wanted with ts.intersect(), I chose to use k = -1. lag(x.ts, k = -1) #shift down one time period(forward) #&gt; Time Series: #&gt; Start = 2 #&gt; End = 101 #&gt; Frequency = 1 #&gt; [1] 0.04172680 0.37190682 -0.18545185 -1.38297422 #&gt; [5] -2.87593652 -2.60017605 -1.10401719 -0.46385116 #&gt; [9] 0.80339069 2.11483585 3.39124978 3.86739194 #&gt; [13] 2.12733595 0.67590604 0.71429367 0.38928044 #&gt; [17] -1.22681923 0.02287443 -0.57321924 -2.68376851 #&gt; [21] -4.80850095 -2.44797633 -1.73921817 -1.48773023 #&gt; [25] 0.68920966 0.40220308 -1.58781041 -2.07300848 #&gt; [29] -2.63408197 -3.54333559 -2.74116328 -2.54401750 #&gt; [33] -3.19570817 -0.02623669 0.41663974 -1.74485791 #&gt; [37] -3.04847994 -1.64692948 -1.71478199 0.11340965 #&gt; [41] 1.55017869 0.47317192 -0.18521791 -0.05759091 #&gt; [45] -1.32105323 -1.22071881 -1.53827085 0.01277076 #&gt; [49] -3.19955388 -3.11022833 -3.30621969 -2.00546537 #&gt; [53] 0.18084415 0.58776479 -0.70238414 -0.34570939 #&gt; [57] 0.94209248 -0.78176791 1.30547320 -1.04054783 #&gt; [61] -1.06897160 -1.08850000 0.06031172 -0.05724856 #&gt; [65] -1.14731083 -0.79262221 -0.55565451 -1.55985750 #&gt; [69] -2.17062644 -1.07776017 0.51569067 2.30660050 #&gt; [73] 1.53530426 2.55899301 1.83836277 1.08072014 #&gt; [77] 1.34125182 -0.80729300 -1.42735924 -0.42456207 #&gt; [81] -0.11625003 -0.74807460 0.70052717 0.08557377 #&gt; [85] -0.06039041 0.04479407 -0.12657328 -1.30097021 #&gt; [89] 0.81586192 -0.13139757 1.84725644 1.62364752 #&gt; [93] 0.33080663 -0.40824385 -1.56008530 -1.63175408 #&gt; [97] -1.36418639 -0.37209392 -0.65833401 2.03705932 lag(x.ts, k = 0) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] 0.04172680 0.37190682 -0.18545185 -1.38297422 #&gt; [5] -2.87593652 -2.60017605 -1.10401719 -0.46385116 #&gt; [9] 0.80339069 2.11483585 3.39124978 3.86739194 #&gt; [13] 2.12733595 0.67590604 0.71429367 0.38928044 #&gt; [17] -1.22681923 0.02287443 -0.57321924 -2.68376851 #&gt; [21] -4.80850095 -2.44797633 -1.73921817 -1.48773023 #&gt; [25] 0.68920966 0.40220308 -1.58781041 -2.07300848 #&gt; [29] -2.63408197 -3.54333559 -2.74116328 -2.54401750 #&gt; [33] -3.19570817 -0.02623669 0.41663974 -1.74485791 #&gt; [37] -3.04847994 -1.64692948 -1.71478199 0.11340965 #&gt; [41] 1.55017869 0.47317192 -0.18521791 -0.05759091 #&gt; [45] -1.32105323 -1.22071881 -1.53827085 0.01277076 #&gt; [49] -3.19955388 -3.11022833 -3.30621969 -2.00546537 #&gt; [53] 0.18084415 0.58776479 -0.70238414 -0.34570939 #&gt; [57] 0.94209248 -0.78176791 1.30547320 -1.04054783 #&gt; [61] -1.06897160 -1.08850000 0.06031172 -0.05724856 #&gt; [65] -1.14731083 -0.79262221 -0.55565451 -1.55985750 #&gt; [69] -2.17062644 -1.07776017 0.51569067 2.30660050 #&gt; [73] 1.53530426 2.55899301 1.83836277 1.08072014 #&gt; [77] 1.34125182 -0.80729300 -1.42735924 -0.42456207 #&gt; [81] -0.11625003 -0.74807460 0.70052717 0.08557377 #&gt; [85] -0.06039041 0.04479407 -0.12657328 -1.30097021 #&gt; [89] 0.81586192 -0.13139757 1.84725644 1.62364752 #&gt; [93] 0.33080663 -0.40824385 -1.56008530 -1.63175408 #&gt; [97] -1.36418639 -0.37209392 -0.65833401 2.03705932 lag(x.ts, k = 1) #&gt; Time Series: #&gt; Start = 0 #&gt; End = 99 #&gt; Frequency = 1 #&gt; [1] 0.04172680 0.37190682 -0.18545185 -1.38297422 #&gt; [5] -2.87593652 -2.60017605 -1.10401719 -0.46385116 #&gt; [9] 0.80339069 2.11483585 3.39124978 3.86739194 #&gt; [13] 2.12733595 0.67590604 0.71429367 0.38928044 #&gt; [17] -1.22681923 0.02287443 -0.57321924 -2.68376851 #&gt; [21] -4.80850095 -2.44797633 -1.73921817 -1.48773023 #&gt; [25] 0.68920966 0.40220308 -1.58781041 -2.07300848 #&gt; [29] -2.63408197 -3.54333559 -2.74116328 -2.54401750 #&gt; [33] -3.19570817 -0.02623669 0.41663974 -1.74485791 #&gt; [37] -3.04847994 -1.64692948 -1.71478199 0.11340965 #&gt; [41] 1.55017869 0.47317192 -0.18521791 -0.05759091 #&gt; [45] -1.32105323 -1.22071881 -1.53827085 0.01277076 #&gt; [49] -3.19955388 -3.11022833 -3.30621969 -2.00546537 #&gt; [53] 0.18084415 0.58776479 -0.70238414 -0.34570939 #&gt; [57] 0.94209248 -0.78176791 1.30547320 -1.04054783 #&gt; [61] -1.06897160 -1.08850000 0.06031172 -0.05724856 #&gt; [65] -1.14731083 -0.79262221 -0.55565451 -1.55985750 #&gt; [69] -2.17062644 -1.07776017 0.51569067 2.30660050 #&gt; [73] 1.53530426 2.55899301 1.83836277 1.08072014 #&gt; [77] 1.34125182 -0.80729300 -1.42735924 -0.42456207 #&gt; [81] -0.11625003 -0.74807460 0.70052717 0.08557377 #&gt; [85] -0.06039041 0.04479407 -0.12657328 -1.30097021 #&gt; [89] 0.81586192 -0.13139757 1.84725644 1.62364752 #&gt; [93] 0.33080663 -0.40824385 -1.56008530 -1.63175408 #&gt; [97] -1.36418639 -0.37209392 -0.65833401 2.03705932 # 創建一個時間序列 x.ts b.ts &lt;- ts(c(10, 20, 30, 40, 50, 60), start = c(2022, 1), frequency = 12) # 使用 ts.intersect() 函數合併 x.ts 和它的三個 lag 時間序列 ts.intersect(b.ts, b.ts1 = lag(x = b.ts, k = -1), b.ts2 = lag(x = b.ts, k = -2), b.ts3 = lag(x = b.ts, k = -3)) #&gt; b.ts b.ts1 b.ts2 b.ts3 #&gt; Apr 2022 40 30 20 10 #&gt; May 2022 50 40 30 20 #&gt; Jun 2022 60 50 40 30 #b.ts1、b.ts2 和 b.ts3 的時間點是 b.ts 的時間點往後推 1、2、3 個時間單位。 b.ts1 = lag(x = b.ts, k = -1) b.ts2 = lag(x = b.ts, k = -2) b.ts3 = lag(x = b.ts, k = -3) b.ts #&gt; Jan Feb Mar Apr May Jun #&gt; 2022 10 20 30 40 50 60 b.ts1 #&gt; Feb Mar Apr May Jun Jul #&gt; 2022 10 20 30 40 50 60 b.ts2 #&gt; Mar Apr May Jun Jul Aug #&gt; 2022 10 20 30 40 50 60 b.ts3 #&gt; Apr May Jun Jul Aug Sep #&gt; 2022 10 20 30 40 50 60 The ts.intersect() function finds the intersection of the four different “variables”. Thecor()function finds the estimated Pearson correlation coefficients between all variable pairs. Notice how close these correlations are to the autocorrelations! The scatterplotMatrix() function finds a scatter plot matrix. The function is in the car package. #Used for estimation later in course gamma.x &lt;- acf(x = x, type = &quot;covariance&quot;, main = expression(paste(&quot;Data simulated from AR(1): &quot;, x[t] == 0.7*x[t-1] + w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;))) gamma.x #&gt; #&gt; Autocovariances of series &#39;x&#39;, by lag #&gt; #&gt; 0 1 2 3 4 5 6 #&gt; 2.5033 1.6864 1.0036 0.4219 -0.0586 -0.3120 -0.1687 #&gt; 7 8 9 10 11 12 13 #&gt; -0.1608 -0.1445 0.0133 -0.1102 -0.1029 -0.0430 0.1599 #&gt; 14 15 16 17 18 19 20 #&gt; 0.1892 0.3997 0.4790 0.3529 0.2015 0.0140 -0.3293 mean(x) #&gt; [1] -0.4963419 Example 7.2 OSU enrollment data Click here to download data. osu.enroll &lt;- read.csv(file = &quot;OSU_enroll.csv&quot;, stringsAsFactors = TRUE) head(osu.enroll) #&gt; t Semester Year Enrollment date #&gt; 1 1 Fall 1989 20110 8/31/1989 #&gt; 2 2 Spring 1990 19128 2/1/1990 #&gt; 3 3 Summer 1990 7553 6/1/1990 #&gt; 4 4 Fall 1990 19591 8/31/1990 #&gt; 5 5 Spring 1991 18361 2/1/1991 #&gt; 6 6 Summer 1991 6702 6/1/1991 tail(osu.enroll) #&gt; t Semester Year Enrollment date #&gt; 35 35 Spring 2001 20004 2/1/2001 #&gt; 36 36 Summer 2001 7558 6/1/2001 #&gt; 37 37 Fall 2001 21872 8/31/2001 #&gt; 38 38 Spring 2002 20922 2/1/2002 #&gt; 39 39 Summer 2002 7868 6/1/2002 #&gt; 40 40 Fall 2002 22992 8/31/2002 x &lt;- osu.enroll$Enrollment rho.x &lt;- acf(x = x, type = &quot;correlation&quot;, main = &quot;OSU Enrollment series&quot;) rho.x #&gt; #&gt; Autocorrelations of series &#39;x&#39;, by lag #&gt; #&gt; 0 1 2 3 4 5 6 7 #&gt; 1.000 -0.470 -0.425 0.909 -0.438 -0.395 0.822 -0.403 #&gt; 8 9 10 11 12 13 14 15 #&gt; -0.358 0.739 -0.367 -0.327 0.655 -0.337 -0.297 0.581 #&gt; 16 #&gt; -0.309 rho.x$acf[1:9] #&gt; [1] 1.0000000 -0.4702315 -0.4253427 0.9087421 -0.4377336 #&gt; [6] -0.3946048 0.8224660 -0.4025871 -0.3584216 Notes: There are some large autocorrelations. This is a characteristic of a nonstationary series (seasonal factor). We will examine this more later. Because the series is not stationary, the hypothesis test for \\(\\rho(h)\\) = 0 should not be done here using the methods discussed earlier. There is a pattern among the autocorrelations. What does this correspond to? (seasonal factor) (similar value/behavior happens during specific period of time/months across different years) "],["resolving-non-stationarity-problems.html", "Chapter 8 Resolving Non-Stationarity Problems 8.1 Differencing 8.2 Backshift Operator 8.3 Fractional Differencing 8.4 Transformations", " Chapter 8 Resolving Non-Stationarity Problems 8.1 Differencing Differencing helps to create the constant mean needed for stationarity. We will use differencing a lot! 1st differences: \\(x_t – x_{t-1} = \\nabla x_t\\) 2nd differences: \\((x_t – x_{t-1}) – (x_{t-1} – x_{t-2}) = \\nabla x_t – \\nabla x_{t-1} = \\nabla^2x_t\\) Taking “differences” between successive data values in the time series helps to remove trend. Specifically, 1st differences help remove linear trend and 2nd differences help remove quadratic trend. Why does this work? Consider the linear trend model \\(x_t = \\beta_0 + \\beta_1t\\) where t = time and \\(\\beta_1 \\ne\\) 0. Then \\[x_t – x_{t-1} = \\beta_0 + \\beta_1t – [\\beta_0 + \\beta_1(t – 1)] = \\beta_1\\] which is not dependent on t. Example 8.1 Non-stationarity in the mean Click Here to download data. Below is the code for a plot of xt vs. t and the ACF for xt. nonstat.mean &lt;- read.csv(file=&quot;nonstat.mean.csv&quot;) head(nonstat.mean) #&gt; time x #&gt; 1 1 1.31 #&gt; 2 2 13.67 #&gt; 3 3 6.29 #&gt; 4 4 -0.95 #&gt; 5 5 9.59 #&gt; 6 6 -0.45 tail(nonstat.mean) #&gt; time x #&gt; 95 95 92.69 #&gt; 96 96 91.22 #&gt; 97 97 100.00 #&gt; 98 98 102.80 #&gt; 99 99 93.82 #&gt; 100 100 108.72 dev.new(width=8, height=6, pointsize=10) plot(x=nonstat.mean$x, ylab=expression(x[t]), xlab=&quot;t (time)&quot;, type=&quot;l&quot;, col=&quot;red&quot;, main=&quot;Nonstationary time series&quot;, panel.first=grid(col=&quot;gray&quot;, lty=&quot;dotted&quot;)) points(x=nonstat.mean$x, col=&quot;blue&quot;, pch=20) acf(x=nonstat.mean$x, type=&quot;correlation&quot;, main=&quot;Plot of the ACF&quot;) # scatter plot of x_t vs. x_t-1 x.ts &lt;- ts(nonstat.mean[,2]) set1 &lt;- ts.intersect(x.ts, x.ts1=lag(x=x.ts, k=-1)) head(set1) #&gt; x.ts x.ts1 #&gt; [1,] 13.67 1.31 #&gt; [2,] 6.29 13.67 #&gt; [3,] -0.95 6.29 #&gt; [4,] 9.59 -0.95 #&gt; [5,] -0.45 9.59 #&gt; [6,] 17.22 -0.45 cor(set1) #&gt; x.ts x.ts1 #&gt; x.ts 1.000000 0.857779 #&gt; x.ts1 0.857779 1.000000 # Need as.numeric() so that plot.ts() is not run, want plot.default() plot(y=as.numeric(set1[,1]), x=as.numeric(set1[,2]), ylab=expression(x[t]), type=&quot;p&quot;, xlab=expression(x[t-1])) This data is said to have “nonstationarity in the mean” because the mean of \\(x_t, \\mu_t,\\) appears to be changing as a function of time. Why is there large positive autocorrelation at lag = 1, 2, … ? Below is the code to find the first differences: # Find first differences first.diff &lt;- diff(x=nonstat.mean$x, lag=1, differences=1) first.diff[1:5] #&gt; [1] 12.36 -7.38 -7.24 10.54 -10.04 nonstat.mean$x[2]-nonstat.mean$x[1] #&gt; [1] 12.36 nonstat.mean$x[3]-nonstat.mean$x[2] #&gt; [1] -7.38 plot(x= first.diff, ylab=expression(x[t]-x[t-1]), xlab=&quot;t (time)&quot;, type=&quot;l&quot;, col=&quot;red&quot;, main=&quot;First differences&quot;, panel.first = grid(col=&quot;gray&quot;,lty=&quot;dotted&quot;)) points(x=first.diff, col=&quot;blue&quot;, pch=20) acf(x=first.diff, type=&quot;correlation&quot;, main=&quot;Plot of the ACF for first differences&quot;) If you want xt and xt - xt-1 in the same data frame, use the ts.intersect() function: x &lt;- ts(data=nonstat.mean$x) x.diff1 &lt;- ts(data=first.diff, start=2) ts.intersect(x, x.diff1) #&gt; Time Series: #&gt; Start = 2 #&gt; End = 100 #&gt; Frequency = 1 #&gt; x x.diff1 #&gt; 2 13.67 12.36 #&gt; 3 6.29 -7.38 #&gt; 4 -0.95 -7.24 #&gt; 5 9.59 10.54 #&gt; 6 -0.45 -10.04 #&gt; 7 17.22 17.67 #&gt; 8 -6.88 -24.10 #&gt; 9 10.36 17.24 #&gt; 10 12.53 2.17 #&gt; 11 4.22 -8.31 #&gt; 12 17.19 12.97 #&gt; 13 26.02 8.83 #&gt; 14 19.14 -6.88 #&gt; 15 23.55 4.41 #&gt; 16 30.05 6.50 #&gt; 17 21.61 -8.44 #&gt; 18 26.86 5.25 #&gt; 19 36.59 9.73 #&gt; 20 10.67 -25.92 #&gt; 21 26.58 15.91 #&gt; 22 19.81 -6.77 #&gt; 23 31.54 11.73 #&gt; 24 22.74 -8.80 #&gt; 25 37.87 15.13 #&gt; 26 35.86 -2.01 #&gt; 27 48.18 12.32 #&gt; 28 36.16 -12.02 #&gt; 29 37.10 0.94 #&gt; 30 43.03 5.93 #&gt; 31 27.79 -15.24 #&gt; 32 33.05 5.26 #&gt; 33 22.76 -10.29 #&gt; 34 36.72 13.96 #&gt; 35 43.67 6.95 #&gt; 36 31.76 -11.91 #&gt; 37 37.28 5.52 #&gt; 38 46.81 9.53 #&gt; 39 37.46 -9.35 #&gt; 40 46.50 9.04 #&gt; 41 35.61 -10.89 #&gt; 42 48.56 12.95 #&gt; 43 43.68 -4.88 #&gt; 44 46.54 2.86 #&gt; 45 62.40 15.86 #&gt; 46 28.79 -33.61 #&gt; 47 47.88 19.09 #&gt; 48 49.36 1.48 #&gt; 49 77.58 28.22 #&gt; 50 41.47 -36.11 #&gt; 51 54.95 13.48 #&gt; 52 24.10 -30.85 #&gt; 53 27.68 3.58 #&gt; 54 55.36 27.68 #&gt; 55 41.44 -13.92 #&gt; 56 76.86 35.42 #&gt; 57 51.43 -25.43 #&gt; 58 41.03 -10.40 #&gt; 59 51.94 10.91 #&gt; 60 47.14 -4.80 #&gt; 61 54.05 6.91 #&gt; 62 48.72 -5.33 #&gt; 63 53.41 4.69 #&gt; 64 69.12 15.71 #&gt; 65 83.26 14.14 #&gt; 66 62.53 -20.73 #&gt; 67 76.77 14.24 #&gt; 68 52.98 -23.79 #&gt; 69 61.83 8.85 #&gt; 70 67.64 5.81 #&gt; 71 66.08 -1.56 #&gt; 72 80.28 14.20 #&gt; 73 87.40 7.12 #&gt; 74 88.11 0.71 #&gt; 75 65.85 -22.26 #&gt; 76 82.29 16.44 #&gt; 77 97.92 15.63 #&gt; 78 79.62 -18.30 #&gt; 79 81.30 1.68 #&gt; 80 58.63 -22.67 #&gt; 81 89.71 31.08 #&gt; 82 85.11 -4.60 #&gt; 83 70.66 -14.45 #&gt; 84 74.24 3.58 #&gt; 85 74.27 0.03 #&gt; 86 66.52 -7.75 #&gt; 87 85.93 19.41 #&gt; 88 95.23 9.30 #&gt; 89 87.26 -7.97 #&gt; 90 74.88 -12.38 #&gt; 91 73.44 -1.44 #&gt; 92 89.44 16.00 #&gt; 93 62.11 -27.33 #&gt; 94 90.01 27.90 #&gt; 95 92.69 2.68 #&gt; 96 91.22 -1.47 #&gt; 97 100.00 8.78 #&gt; 98 102.80 2.80 #&gt; 99 93.82 -8.98 #&gt; 100 108.72 14.90 Why does the data set start at 2? Other types of differencing: 2nd differences: diff(x, lag = 1, differences = 2) xt – xt-2: diff(x, lag = 2, differences = 1); this can be useful when there is a “seasonal” trend # Second differences diff(x, lag = 1, differences=2) #&gt; Time Series: #&gt; Start = 3 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] -19.74 0.14 17.78 -20.58 27.71 -41.77 41.34 -15.07 #&gt; [9] -10.48 21.28 -4.14 -15.71 11.29 2.09 -14.94 13.69 #&gt; [17] 4.48 -35.65 41.83 -22.68 18.50 -20.53 23.93 -17.14 #&gt; [25] 14.33 -24.34 12.96 4.99 -21.17 20.50 -15.55 24.25 #&gt; [33] -7.01 -18.86 17.43 4.01 -18.88 18.39 -19.93 23.84 #&gt; [41] -17.83 7.74 13.00 -49.47 52.70 -17.61 26.74 -64.33 #&gt; [49] 49.59 -44.33 34.43 24.10 -41.60 49.34 -60.85 15.03 #&gt; [57] 21.31 -15.71 11.71 -12.24 10.02 11.02 -1.57 -34.87 #&gt; [65] 34.97 -38.03 32.64 -3.04 -7.37 15.76 -7.08 -6.41 #&gt; [73] -22.97 38.70 -0.81 -33.93 19.98 -24.35 53.75 -35.68 #&gt; [81] -9.85 18.03 -3.55 -7.78 27.16 -10.11 -17.27 -4.41 #&gt; [89] 10.94 17.44 -43.33 55.23 -25.22 -4.15 10.25 -5.98 #&gt; [97] -11.78 23.88 (nonstat.mean$x[3] - nonstat.mean$x[2]) - (nonstat.mean$x[2] - nonstat.mean$x[1]) #&gt; [1] -19.74 # x_t - x_t-2 diff(x, lag = 2, differences = 1) #&gt; Time Series: #&gt; Start = 3 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] 4.98 -14.62 3.30 0.50 7.63 -6.43 -6.86 19.41 #&gt; [9] -6.14 4.66 21.80 1.95 -2.47 10.91 -1.94 -3.19 #&gt; [17] 14.98 -16.19 -10.01 9.14 4.96 2.93 6.33 13.12 #&gt; [25] 10.31 0.30 -11.08 6.87 -9.31 -9.98 -5.03 3.67 #&gt; [33] 20.91 -4.96 -6.39 15.05 0.18 -0.31 -1.85 2.06 #&gt; [41] 8.07 -2.02 18.72 -17.75 -14.52 20.57 29.70 -7.89 #&gt; [49] -22.63 -17.37 -27.27 31.26 13.76 21.50 9.99 -35.83 #&gt; [57] 0.51 6.11 2.11 1.58 -0.64 20.40 29.85 -6.59 #&gt; [65] -6.49 -9.55 -14.94 14.66 4.25 12.64 21.32 7.83 #&gt; [73] -21.55 -5.82 32.07 -2.67 -16.62 -20.99 8.41 26.48 #&gt; [81] -19.05 -10.87 3.61 -7.72 11.66 28.71 1.33 -20.35 #&gt; [89] -13.82 14.56 -11.33 0.57 30.58 1.21 7.31 11.58 #&gt; [97] -6.18 5.92 nonstat.mean$x[3] - nonstat.mean$x[1] #&gt; [1] 4.98 # Plot lag.plot(x, lags = 4, layout = c(2,2), do.lines = FALSE) Note: There are formal hypothesis tests to determine if differencing is needed. This corresponds to an area of time series known as “unit root” testing. The name will be clear once we examine autoregressive models in detail. 8.2 Backshift Operator A convenient way to represent differencing in time series models is to use the “backshift operator”. It is denoted by “B” and defined as follows: \\[Bx_t=x_{t-1}\\] Notice that \\(x_t\\) moved back one-time period when the backshift operator was applied to it. In general, \\(B^2x_t = x_{t-2}, B^3x_t = x_{t-3}, …, and \\quad B^kx_t = x_{t-k}.\\) Notes: Let C be a constant not indexed by time. Then \\(BC = C.\\) \\((1-B)x_t = x_t – x_{t-1} = \\nabla x_t\\) \\(B\\times B = B^2\\) \\[(1-B)^2x_t = (1 - 2B + B^2)x_t = x_t – 2Bx_t + B^2x_t\\\\ = x_t – 2x_{t-1} + x_{t-2}\\\\ = x_t – x_{t-1} – x_{t-1} + x_{t-2}\\\\ = (x_t – x_{t-1}) – (x_{t-1} – x_{t-2})\\\\ = \\nabla^2x_t\\] \\((1-B)^0x_t = x_t\\) \\((1-B)x_t\\) can be thought of as a “linear filter” since the linear trend is being filtered out of the time series. Example 8.2 Moving Average \\(m_t=\\frac{w_t+w_{t-1}+w_{t-2}}{3}\\), where \\(w_t \\sim \\mathrm{ind}N(0,\\sigma^2_w) \\forall t=1,..n\\) can be represented by \\((1+B+B^2)\\frac{w_t}{3}\\) Example 8.3 Autoregression \\(x_t=0.7x_{t-1}+w_t\\), where \\(w_t \\sim \\mathrm{ind}N(0,\\sigma^2_w) \\forall t=1,..n\\) can be represented by \\((1-0.7B)x_t=w_t\\) Example 8.4 first differencing needed example Consider the following model: \\((1-0.7B)(1-B)x_t=w_t\\), where \\(w_t \\sim \\mathrm{ind}N(0,\\sigma^2_w) \\forall t=1,..n\\). This simplifies to \\[(1-0.7B)(x_t-x_{t-1})=w_t\\\\ \\iff x_t-x_{t-1}-0.7Bx_t+0.7Bx_{t-1}=w_t\\\\ \\iff x_t=x_{t-1}+0.7x_{t-1}-0.7x_{t-2}+w_t\\\\ \\iff x_t=1.7x_{t-1}-0.7x_{t-1}+w_t\\] Later in the course, we will identify this as a ARIMA(1,1,0) model. Suppose a realization of a time series is simulated from this model. Below is a plot of the data. set.seed(7328) w &lt;- rnorm(n = 200, mean = 0, sd = 1) x &lt;- numeric(length = 200) x.1 &lt;- 0 x.2 &lt;- 0 for (i in 1:length(x)) { x[i] &lt;- 1.7*x.1 - 0.7*x.2 + w[i] x.2 &lt;- x.1 x.1 &lt;- x[i] } #Do not use first 100 X &lt;- x[101:200] dev.new(width = 8, height = 6, pointsize = 10) plot(x = X, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 , main = expression(paste(&quot;Data simulated from &quot;, (1-0.7*B)*(1- B)*x[t] == w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;)), panel.first=grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = X, pch = 20, col = &quot;blue&quot;) acf(x=X, type=&quot;correlation&quot;, main=&quot;Plot of the ACF&quot;) After the first differences: # Finf first differences plot(x=diff(x=X, lag=1, differences = 1), ylab=expression(x[t]-x[t-1]), xlab=&quot;t (time)&quot;, type=&quot;l&quot;, col=&quot;red&quot;, main=expression(paste(&quot;1st diff. for data simulated from &quot;, (1-0.7*B)*(1-B)*x[t] == w[t], &quot; where &quot;, w[t], &quot;~N(0,1)&quot;)), panel.first=grid(col=&quot;gray&quot;,lty=&quot;dotted&quot;)) points(x=diff(x=X, lag=1, differences = 1), pch=20, col=&quot;blue&quot;) acf(x=diff(x=X, lag=1, differences = 1), type=&quot;correlation&quot;, main=&quot;Plot of the ACF&quot;) An easier way is to use arima.sim() to dimulate the data. 8.3 Fractional Differencing Use fractional powers of B between –0.5 to 0.5 to do the differencing. This is used with long-memory time series. Notes: Differencing is often used to help make a nonstationary in the mean time series stationary. Unfortunately in real applications, we do not know what exact level of differencing is needed (we can approximate it). If a too high of level of differencing is done, this can hurt a time series model. As a compromise between differencing and not differencing at all, fractional differencing can be used. The reason why these are called a “long memory” time series can be seen from a “short memory” time series. A short memory stationary process will have \\(\\rho(h)\\to 0\\) “quickly” as \\(h\\to \\infty\\). A long memory time series does not and has \\(\\rho(h)\\to 0\\) “slowly”. More on this later in the course. The fractional difference series can be represented as \\[\\nabla^dx_t=\\sum_{j=0}^{\\infty}\\pi_jx_{t-j}\\] where the \\(\\pi_j\\) are found through a Taylor series expansion of \\((1-B)^d\\). 8.4 Transformations In regression analysis, transformations of the response variable are taken to induce approximate constant variance. In a similar manner, we can take transformations of xt to help make a nonstationary in the variance time series be approximately stationary in the variance. Example 8.5 Johnson &amp; Johnson earnings per share data This data comes from Shumway and Stoffer’s book. library(astsa) x &lt;- jj x #&gt; Qtr1 Qtr2 Qtr3 Qtr4 #&gt; 1960 0.710000 0.630000 0.850000 0.440000 #&gt; 1961 0.610000 0.690000 0.920000 0.550000 #&gt; 1962 0.720000 0.770000 0.920000 0.600000 #&gt; 1963 0.830000 0.800000 1.000000 0.770000 #&gt; 1964 0.920000 1.000000 1.240000 1.000000 #&gt; 1965 1.160000 1.300000 1.450000 1.250000 #&gt; 1966 1.260000 1.380000 1.860000 1.560000 #&gt; 1967 1.530000 1.590000 1.830000 1.860000 #&gt; 1968 1.530000 2.070000 2.340000 2.250000 #&gt; 1969 2.160000 2.430000 2.700000 2.250000 #&gt; 1970 2.790000 3.420000 3.690000 3.600000 #&gt; 1971 3.600000 4.320000 4.320000 4.050000 #&gt; 1972 4.860000 5.040000 5.040000 4.410000 #&gt; 1973 5.580000 5.850000 6.570000 5.310000 #&gt; 1974 6.030000 6.390000 6.930000 5.850000 #&gt; 1975 6.930000 7.740000 7.830000 6.120000 #&gt; 1976 7.740000 8.910000 8.280000 6.840000 #&gt; 1977 9.540000 10.260000 9.540000 8.729999 #&gt; 1978 11.880000 12.060000 12.150000 8.910000 #&gt; 1979 14.040000 12.960000 14.850000 9.990000 #&gt; 1980 16.200000 14.670000 16.020000 11.610000 dev.new(width = 8, height = 6, pointsize = 10) #Opens up wider plot window than the default (good for time series plots) plot(x = x, ylab = expression(x[t]), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1, main = &quot;Johnson and Johnson quarterly earnings per share&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = x, pch = 20, col = &quot;blue&quot;) plot(x = log(x), ylab = expression(log(x[t])), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 , main = &quot;Johnson and Johnson quarterly earnings per share - log transformed&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = log(x), pch = 20, col = &quot;blue&quot;) #Still some variance issues??? plot(x = diff(x = log(x), lag = 1, differences = 1), ylab = expression(log(x[t]) - log(x[t-1])), xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 , main = &quot;Johnson and Johnson quarterly earnings per share - log transformed and 1st diff.&quot;, panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = diff(x = log(x), lag = 1, differences = 1), pch = 20, col = &quot;blue&quot;) Notes: Typical transformations include \\(log(x_t),\\sqrt{x_t}\\) and \\(x_t^{-1}\\) There are a few different ways to deciding between the appropriate transformations. Usually, I will try all of these transformations and examine a plot of the transformed data over time to determine if the transformation worked. Constants may need to be added to \\(x_t\\) if \\(x_t\\) can be less than 0. Regression courses sometimes teach the Box-Cox family of transformations to determine an appropriate transformation. The process involves finding the “best” \\(\\lambda\\) to transform \\(x_t\\) in the following manner: \\[ y_t= \\begin{cases} (x^{\\lambda}_t-1)/\\lambda &amp; \\text{if } \\lambda \\ne 0 \\\\ log(x_t) &amp; \\text{if } \\lambda=0 \\\\ \\end{cases} \\] I have not found it used often in time series analysis. For example, Shumway and Stoffer suggest it could be used here, but do not explore it further. The BoxCox() and BoxCox.lambda() functions from the forecast package provide ways to obtain it. Please see my program. #How could one do a Box-Cox transformation here? library(package = forecast) #&gt; Registered S3 method overwritten by &#39;quantmod&#39;: #&gt; method from #&gt; as.zoo.data.frame zoo #&gt; #&gt; Attaching package: &#39;forecast&#39; #&gt; The following object is masked from &#39;package:astsa&#39;: #&gt; #&gt; gas save.it &lt;- BoxCox(x = x, lambda = &quot;auto&quot;) # save.it - this has the transformed data names(save.it) # Not useful #&gt; NULL attributes(save.it) # Older form of R coding shows it #&gt; $tsp #&gt; [1] 1960.00 1980.75 4.00 #&gt; #&gt; $class #&gt; [1] &quot;ts&quot; #&gt; #&gt; $lambda #&gt; [1] 0.1540791 attributes(save.it)$lambda #&gt; [1] 0.1540791 # Looking at the code in BoxCox() leads one to use the function below to # obtain lambda BoxCox.lambda(x = x) #&gt; [1] 0.1540752 lambda &lt;- BoxCox.lambda(x = x) # Don&#39;t see a benefit from using this transformation over log() plot(x = (x^lambda - 1)/lambda, ylab = &quot;Transformed x&quot;, xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 ,panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = (x^lambda - 1)/lambda, pch = 20, col = &quot;blue&quot;) plot(x = diff(x = (x^lambda - 1)/lambda, lag = 1, differences = 1), ylab = &quot;Transformed x with first differences&quot;, xlab = &quot;t&quot;, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 1 , panel.first = grid(col = &quot;gray&quot;, lty = &quot;dotted&quot;)) points(x = diff(x = (x^lambda - 1)/lambda, lag = 1, differences = 1), pch = 20, col = &quot;blue&quot;) If the variance stabilizing transformation is needed, do this before differencing (see Wei’s time series book for a discussion). For example, suppose differencing and a natural log variance stabilizing transformation is needed. Then examine \\(log(x_t) – log(x_{t-1})\\) instead of \\(log(x_t – x_{t-1}).\\) Variance stabilizing transformations often help with normality assumption of \\(w_t\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
